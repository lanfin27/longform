# -*- coding: utf-8 -*-
"""
ì±„ë„ íŠ¸ë Œë“œ ë¶„ì„ê¸°

ì—­ì¶”ì  ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ ì‹ ê·œ ì±„ë„ ë°œêµ´ ë° íŠ¸ë Œë“œ ë¶„ì„
YouTube APIëŠ” ì±„ë„ ìƒì„±ì¼ ê¸°ì¤€ ê²€ìƒ‰ì´ ì•ˆ ë˜ë¯€ë¡œ,
"ìµœê·¼ ì˜ìƒ ê²€ìƒ‰ â†’ ì±„ë„ ID ì¶”ì¶œ â†’ ì±„ë„ ìƒì„±ì¼ í•„í„°ë§" ë°©ì‹ ì‚¬ìš©
"""
import os
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from collections import Counter
from dateutil import parser
import hashlib

from googleapiclient.discovery import build

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from config.settings import YOUTUBE_API_KEY
from core.youtube.cache import get_cache


@dataclass
class NewChannel:
    """ì‹ ê·œ ì±„ë„ ì •ë³´"""
    channel_id: str
    title: str
    description: str
    created_at: str  # YYYY-MM-DD
    created_at_dt: datetime
    subscribers: int
    video_count: int
    view_count: int
    thumbnail_url: str
    channel_url: str

    # ê³„ì‚° ì§€í‘œ
    avg_views_per_video: float = 0.0
    subscribers_per_video: float = 0.0  # ì˜ìƒë‹¹ êµ¬ë…ì íš¨ìœ¨
    days_since_creation: int = 0
    growth_rate: str = "ë³´í†µ"  # ê¸‰ì„±ì¥/ë³´í†µ/ì €ì¡°

    # â­ ê¸°íšŒ ì§€ìˆ˜ (Opportunity Score) - í•µì‹¬ ì§€í‘œ!
    opportunity_score: float = 0.0  # í‰ê· ì¡°íšŒìˆ˜ / êµ¬ë…ììˆ˜
    opportunity_label: str = ""  # ê¸°íšŒ ë ˆë²¨ ë¼ë²¨

    # í‚¤ì›Œë“œ ê´€ë ¨ì„± ì§€í‘œ
    relevance_score: int = 0  # 0-10 ì ìˆ˜
    keyword_relevant: bool = False  # í‚¤ì›Œë“œ ì§ì ‘ í¬í•¨ ì—¬ë¶€
    relevance_reason: str = ""  # ê´€ë ¨ì„± ì´ìœ 

    def calculate_metrics(self):
        """ì„±ê³¼ ì§€í‘œ ê³„ì‚°"""
        if self.video_count > 0:
            self.avg_views_per_video = self.view_count / self.video_count
            self.subscribers_per_video = self.subscribers / self.video_count

        self.days_since_creation = (datetime.now() - self.created_at_dt).days

        # â­ ê¸°íšŒ ì§€ìˆ˜ ê³„ì‚°: í‰ê· ì¡°íšŒìˆ˜ / êµ¬ë…ììˆ˜
        # êµ¬ë…ìê°€ ì ì€ë° ì¡°íšŒìˆ˜ê°€ ë†’ë‹¤ = ì•Œê³ ë¦¬ì¦˜ì´ ë°€ì–´ì£¼ëŠ” í‚¤ì›Œë“œ
        if self.subscribers > 0:
            self.opportunity_score = self.avg_views_per_video / self.subscribers
        else:
            self.opportunity_score = self.avg_views_per_video  # êµ¬ë…ì 0ì´ë©´ ì¡°íšŒìˆ˜ ê·¸ëŒ€ë¡œ

        # ê¸°íšŒ ì§€ìˆ˜ ë ˆë²¨ íŒì •
        if self.opportunity_score >= 100:
            self.opportunity_label = "ğŸŒŸ í™©ê¸ˆê¸°íšŒ"
        elif self.opportunity_score >= 50:
            self.opportunity_label = "âœ… ì¢‹ì€ê¸°íšŒ"
        elif self.opportunity_score >= 10:
            self.opportunity_label = "ğŸŸ¡ ë³´í†µ"
        else:
            self.opportunity_label = "ğŸ”´ í¬í™”"

        # ì„±ì¥ë¥  íŒì • (ì˜ìƒë‹¹ êµ¬ë…ì ê¸°ì¤€)
        if self.subscribers_per_video >= 100:
            self.growth_rate = "ğŸš€ ê¸‰ì„±ì¥"
        elif self.subscribers_per_video >= 30:
            self.growth_rate = "ğŸ“ˆ ì–‘í˜¸"
        elif self.subscribers_per_video >= 10:
            self.growth_rate = "â¡ï¸ ë³´í†µ"
        else:
            self.growth_rate = "ğŸ“‰ ì €ì¡°"

    def to_dict(self) -> dict:
        return {
            "channel_id": self.channel_id,
            "title": self.title,
            "description": self.description[:200] if self.description else "",
            "created_at": self.created_at,
            "subscribers": self.subscribers,
            "video_count": self.video_count,
            "view_count": self.view_count,
            "avg_views_per_video": round(self.avg_views_per_video, 1),
            "subscribers_per_video": round(self.subscribers_per_video, 1),
            "days_since_creation": self.days_since_creation,
            "growth_rate": self.growth_rate,
            "channel_url": self.channel_url,
            "thumbnail_url": self.thumbnail_url,
            "opportunity_score": round(self.opportunity_score, 2),
            "opportunity_label": self.opportunity_label,
            "relevance_score": self.relevance_score,
            "keyword_relevant": self.keyword_relevant,
            "relevance_reason": self.relevance_reason
        }


@dataclass
class TrendAnalysisResult:
    """íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼"""
    keyword: str
    region: str
    period_months: int
    analysis_date: str

    total_videos_searched: int
    unique_channels_found: int
    new_channels_count: int

    new_channels: List[NewChannel] = field(default_factory=list)
    monthly_trend: Dict[str, int] = field(default_factory=dict)  # {"2024-01": 3, "2024-02": 5, ...}

    # ìš”ì•½ í†µê³„
    avg_subscribers: float = 0.0
    avg_video_count: float = 0.0
    avg_views: float = 0.0

    # â­ ì‹œì¥ ê¸°íšŒ ì§€í‘œ (Market Opportunity Metrics)
    avg_opportunity_score: float = 0.0  # ì „ì²´ ì±„ë„ í‰ê·  ê¸°íšŒ ì§€ìˆ˜
    market_verdict: str = ""  # blue_ocean, growing, competitive, red_ocean
    market_verdict_label: str = ""  # í•œê¸€ ë¼ë²¨
    supply_index: float = 0.0  # ê²½ìŸ ê°•ë„ (ì›” í‰ê·  ì‹ ê·œ ì±„ë„ ìˆ˜)
    demand_index: float = 0.0  # ìˆ˜ìš” ì§€ìˆ˜ (í‰ê·  ì¡°íšŒìˆ˜ ê¸°ë°˜)

    # AI ì¸ì‚¬ì´íŠ¸
    ai_insight: str = ""

    def calculate_summary(self):
        """ìš”ì•½ í†µê³„ ë° ì‹œì¥ ê¸°íšŒ ì§€í‘œ ê³„ì‚°"""
        if self.new_channels:
            self.avg_subscribers = sum(c.subscribers for c in self.new_channels) / len(self.new_channels)
            self.avg_video_count = sum(c.video_count for c in self.new_channels) / len(self.new_channels)
            self.avg_views = sum(c.view_count for c in self.new_channels) / len(self.new_channels)

            # â­ ì‹œì¥ ê¸°íšŒ ì§€ìˆ˜ ê³„ì‚°
            self._calculate_market_opportunity()

    def _calculate_market_opportunity(self):
        """ì‹œì¥ ê¸°íšŒ ì§€í‘œ ê³„ì‚°"""
        if not self.new_channels:
            return

        # 1. í‰ê·  ê¸°íšŒ ì§€ìˆ˜ (ê´€ë ¨ ì±„ë„ë§Œ ëŒ€ìƒ)
        relevant_channels = [c for c in self.new_channels if c.keyword_relevant]
        if relevant_channels:
            self.avg_opportunity_score = sum(c.opportunity_score for c in relevant_channels) / len(relevant_channels)
        else:
            # ê´€ë ¨ ì±„ë„ì´ ì—†ìœ¼ë©´ ì „ì²´ ì±„ë„ ê¸°ì¤€
            self.avg_opportunity_score = sum(c.opportunity_score for c in self.new_channels) / len(self.new_channels)

        # 2. ê³µê¸‰ ì§€ìˆ˜ (ê²½ìŸ ê°•ë„): ì›” í‰ê·  ì‹ ê·œ ì±„ë„ ìˆ˜
        self.supply_index = self.new_channels_count / max(1, self.period_months)

        # 3. ìˆ˜ìš” ì§€ìˆ˜: í‰ê·  ì¡°íšŒìˆ˜ ê¸°ë°˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼ ì •ê·œí™”)
        avg_views = sum(c.avg_views_per_video for c in self.new_channels) / len(self.new_channels)
        import math
        self.demand_index = math.log10(max(1, avg_views)) * 10  # 0~60 ë²”ìœ„

        # 4. ì‹œì¥ íŒì • (Market Verdict)
        # ê¸°ì¤€: ê¸°íšŒì§€ìˆ˜ + ê³µê¸‰ì§€ìˆ˜ ì¡°í•©
        if self.avg_opportunity_score >= 50:
            if self.supply_index < 3:
                self.market_verdict = "blue_ocean"
                self.market_verdict_label = "ğŸ”µ ë¸”ë£¨ì˜¤ì…˜"
            else:
                self.market_verdict = "growing"
                self.market_verdict_label = "ğŸŸ¢ ì„±ì¥ì‹œì¥"
        elif self.avg_opportunity_score >= 10:
            if self.supply_index < 5:
                self.market_verdict = "growing"
                self.market_verdict_label = "ğŸŸ¢ ì„±ì¥ì‹œì¥"
            else:
                self.market_verdict = "competitive"
                self.market_verdict_label = "ğŸŸ¡ ê²½ìŸì‹œì¥"
        else:
            if self.supply_index >= 5:
                self.market_verdict = "red_ocean"
                self.market_verdict_label = "ğŸ”´ ë ˆë“œì˜¤ì…˜"
            else:
                self.market_verdict = "competitive"
                self.market_verdict_label = "ğŸŸ¡ ê²½ìŸì‹œì¥"

    def get_rising_stars(self, top_n: int = 5) -> List['NewChannel']:
        """ê¸°íšŒ ì§€ìˆ˜ê°€ ë†’ì€ ìƒìœ„ ì±„ë„ (ë¼ì´ì§• ìŠ¤íƒ€) ë°˜í™˜"""
        # ê´€ë ¨ ì±„ë„ ì¤‘ì—ì„œ ê¸°íšŒ ì§€ìˆ˜ ë†’ì€ ìˆœ
        relevant = [c for c in self.new_channels if c.keyword_relevant]
        if not relevant:
            relevant = self.new_channels

        sorted_channels = sorted(relevant, key=lambda x: x.opportunity_score, reverse=True)
        return sorted_channels[:top_n]

    def get_golden_opportunities(self) -> List['NewChannel']:
        """í™©ê¸ˆ ê¸°íšŒ ì±„ë„ë“¤ (opportunity_score >= 100) ë°˜í™˜"""
        return [c for c in self.new_channels if c.opportunity_score >= 100 and c.keyword_relevant]


class ChannelTrendAnalyzer:
    """ì±„ë„ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""

    def __init__(self, api_key: str = None, cache_dir: str = "data/cache/channel_trends"):
        """
        Args:
            api_key: YouTube API í‚¤ (ê¸°ë³¸: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
        """
        self.api_key = api_key or YOUTUBE_API_KEY
        if not self.api_key:
            raise ValueError("YouTube API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

        self.youtube = build('youtube', 'v3', developerKey=self.api_key)
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

        self.cache_expiry_days = 7
        self._cache = get_cache()

    def _get_cache_key(self, keyword: str, region: str, months: int) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_str = f"trend_{keyword}_{region}_{months}"
        return hashlib.md5(key_str.encode()).hexdigest()[:16]

    def _get_cached_result(self, cache_key: str) -> Optional[dict]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")

        if os.path.exists(cache_file):
            stat = os.stat(cache_file)
            age_days = (datetime.now().timestamp() - stat.st_mtime) / 86400

            if age_days < self.cache_expiry_days:
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
                except:
                    pass

        return None

    def _save_cache(self, cache_key: str, data: dict):
        """ê²°ê³¼ ìºì‹±"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        except Exception as e:
            print(f"[ChannelTrend] ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")

    def analyze_channel_trend(
        self,
        keyword: str,
        region: str = "KR",
        months: int = 6,
        max_videos: int = 100,
        use_cache: bool = True,
        progress_callback=None
    ) -> TrendAnalysisResult:
        """
        ì±„ë„ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            region: êµ­ê°€ ì½”ë“œ (KR, JP, US ë“±)
            months: ë¶„ì„ ê¸°ê°„ (ê°œì›”)
            max_videos: ìµœëŒ€ ê²€ìƒ‰ ì˜ìƒ ìˆ˜
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜

        Returns:
            TrendAnalysisResult
        """
        def update_progress(msg):
            if progress_callback:
                progress_callback(msg)
            print(f"[ChannelTrend] {msg}")

        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(keyword, region, months)
        if use_cache:
            cached = self._get_cached_result(cache_key)
            if cached:
                update_progress("ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©")
                return self._dict_to_result(cached)

        # 1. ê¸°ì¤€ ë‚ ì§œ ì„¤ì •
        cutoff_date = datetime.now() - timedelta(days=months * 30)
        published_after = cutoff_date.isoformat() + "Z"

        # 2. ìµœê·¼ ì˜ìƒ ê²€ìƒ‰ (Step 1)
        update_progress(f"í‚¤ì›Œë“œ '{keyword}' ì˜ìƒ ê²€ìƒ‰ ì¤‘...")

        all_videos = []
        next_page_token = None

        while len(all_videos) < max_videos:
            try:
                search_response = self.youtube.search().list(
                    q=keyword,
                    part="snippet",
                    type="video",
                    order="date",  # ìµœì‹ ìˆœ
                    publishedAfter=published_after,
                    regionCode=region,
                    maxResults=min(50, max_videos - len(all_videos)),
                    pageToken=next_page_token
                ).execute()

                # í• ë‹¹ëŸ‰ ê¸°ë¡
                self._cache.log_api_call("search")

                all_videos.extend(search_response.get('items', []))
                next_page_token = search_response.get('nextPageToken')

                if not next_page_token:
                    break

            except Exception as e:
                update_progress(f"ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                break

        update_progress(f"ì´ {len(all_videos)}ê°œ ì˜ìƒ ê²€ìƒ‰ë¨")

        if not all_videos:
            return TrendAnalysisResult(
                keyword=keyword,
                region=region,
                period_months=months,
                analysis_date=datetime.now().strftime("%Y-%m-%d %H:%M"),
                total_videos_searched=0,
                unique_channels_found=0,
                new_channels_count=0,
                new_channels=[],
                monthly_trend={}
            )

        # 3. ì±„ë„ ID ì¶”ì¶œ (Step 2)
        channel_ids = list(set([
            item['snippet']['channelId']
            for item in all_videos
            if 'channelId' in item['snippet']
        ]))

        update_progress(f"ê³ ìœ  ì±„ë„ {len(channel_ids)}ê°œ ë°œê²¬")

        # 4. ì±„ë„ ìƒì„¸ ì •ë³´ ì¡°íšŒ (Step 3)
        update_progress("ì±„ë„ ì •ë³´ ì¡°íšŒ ì¤‘...")
        all_channel_data = []

        for i in range(0, len(channel_ids), 50):
            batch_ids = channel_ids[i:i+50]

            try:
                channels_response = self.youtube.channels().list(
                    part="snippet,statistics",
                    id=",".join(batch_ids)
                ).execute()

                # í• ë‹¹ëŸ‰ ê¸°ë¡
                self._cache.log_api_call("channels")

                all_channel_data.extend(channels_response.get('items', []))

            except Exception as e:
                update_progress(f"ì±„ë„ ì¡°íšŒ ì˜¤ë¥˜: {e}")

        # 5. ì±„ë„ ìƒì„±ì¼ í•„í„°ë§ + í‚¤ì›Œë“œ ê´€ë ¨ì„± ê³„ì‚° (Step 4) - í•µì‹¬!
        update_progress("ì‹ ê·œ ì±„ë„ í•„í„°ë§ ë° ê´€ë ¨ì„± ë¶„ì„ ì¤‘...")
        new_channels: List[NewChannel] = []
        monthly_counter = Counter()

        # í‚¤ì›Œë“œ ë³€í˜• ì¤€ë¹„ (ê´€ë ¨ì„± ê²€ì‚¬ìš©)
        keyword_lower = keyword.lower()
        keyword_variants = self._get_keyword_variants(keyword)

        for ch in all_channel_data:
            try:
                # ì±„ë„ ìƒì„±ì¼ íŒŒì‹±
                created_at_str = ch['snippet']['publishedAt']
                created_at = parser.parse(created_at_str).replace(tzinfo=None)

                # ì‹ ê·œ ì±„ë„ í•„í„°ë§ (ê¸°ì¤€ì¼ ì´í›„ ìƒì„±)
                if created_at > cutoff_date:
                    stats = ch.get('statistics', {})

                    # ë¹„ê³µê°œ í†µê³„ ì²˜ë¦¬
                    subscribers = int(stats.get('subscriberCount', 0))
                    video_count = int(stats.get('videoCount', 0))
                    view_count = int(stats.get('viewCount', 0))

                    title = ch['snippet']['title']
                    description = ch['snippet'].get('description', '')

                    new_channel = NewChannel(
                        channel_id=ch['id'],
                        title=title,
                        description=description,
                        created_at=created_at.strftime('%Y-%m-%d'),
                        created_at_dt=created_at,
                        subscribers=subscribers,
                        video_count=video_count,
                        view_count=view_count,
                        thumbnail_url=ch['snippet']['thumbnails']['default']['url'],
                        channel_url=f"https://youtube.com/channel/{ch['id']}"
                    )

                    new_channel.calculate_metrics()

                    # â­ í‚¤ì›Œë“œ ê´€ë ¨ì„± ê³„ì‚° (í•µì‹¬!)
                    relevance_score, is_relevant, reason = self._calculate_keyword_relevance(
                        title, description, keyword_variants
                    )
                    new_channel.relevance_score = relevance_score
                    new_channel.keyword_relevant = is_relevant
                    new_channel.relevance_reason = reason

                    new_channels.append(new_channel)

                    # ì›”ë³„ ì¹´ìš´íŠ¸
                    month_key = created_at.strftime('%Y-%m')
                    monthly_counter[month_key] += 1

            except Exception as e:
                print(f"[ChannelTrend] ì±„ë„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        # 6. ê²°ê³¼ ì •ë ¬ (ê´€ë ¨ì„± ë†’ì€ ìˆœ â†’ ìµœì‹ ìˆœ)
        # ê´€ë ¨ì„± ì ìˆ˜ê°€ ë†’ì€ ì±„ë„ì´ ë¨¼ì €, ê°™ìœ¼ë©´ ìµœì‹ ìˆœ
        new_channels.sort(key=lambda x: (-x.relevance_score, -x.created_at_dt.timestamp()))

        # ì›”ë³„ íŠ¸ë Œë“œ ì •ë ¬
        monthly_trend = dict(sorted(monthly_counter.items()))

        # ê´€ë ¨ì„± í†µê³„ ë¡œê¹…
        relevant_count = len([c for c in new_channels if c.keyword_relevant])
        update_progress(f"ì‹ ê·œ ì±„ë„ {len(new_channels)}ê°œ ë°œê²¬ (í‚¤ì›Œë“œ ê´€ë ¨: {relevant_count}ê°œ)")

        # 7. ê²°ê³¼ ìƒì„±
        result = TrendAnalysisResult(
            keyword=keyword,
            region=region,
            period_months=months,
            analysis_date=datetime.now().strftime("%Y-%m-%d %H:%M"),
            total_videos_searched=len(all_videos),
            unique_channels_found=len(channel_ids),
            new_channels_count=len(new_channels),
            new_channels=new_channels,
            monthly_trend=monthly_trend
        )

        result.calculate_summary()

        # ìºì‹±
        self._save_cache(cache_key, self._result_to_dict(result))

        return result

    def _result_to_dict(self, result: TrendAnalysisResult) -> dict:
        """ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ìºì‹±ìš©)"""
        return {
            "keyword": result.keyword,
            "region": result.region,
            "period_months": result.period_months,
            "analysis_date": result.analysis_date,
            "total_videos_searched": result.total_videos_searched,
            "unique_channels_found": result.unique_channels_found,
            "new_channels_count": result.new_channels_count,
            "new_channels": [c.to_dict() for c in result.new_channels],
            "monthly_trend": result.monthly_trend,
            "avg_subscribers": result.avg_subscribers,
            "avg_video_count": result.avg_video_count,
            "avg_views": result.avg_views,
            "avg_opportunity_score": result.avg_opportunity_score,
            "market_verdict": result.market_verdict,
            "market_verdict_label": result.market_verdict_label,
            "supply_index": result.supply_index,
            "demand_index": result.demand_index,
            "ai_insight": result.ai_insight
        }

    def _dict_to_result(self, data: dict) -> TrendAnalysisResult:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ ê²°ê³¼ ê°ì²´ë¡œ ë³€í™˜"""
        new_channels = []
        for ch_data in data.get("new_channels", []):
            try:
                ch = NewChannel(
                    channel_id=ch_data["channel_id"],
                    title=ch_data["title"],
                    description=ch_data.get("description", ""),
                    created_at=ch_data["created_at"],
                    created_at_dt=datetime.strptime(ch_data["created_at"], "%Y-%m-%d"),
                    subscribers=ch_data["subscribers"],
                    video_count=ch_data["video_count"],
                    view_count=ch_data["view_count"],
                    thumbnail_url=ch_data.get("thumbnail_url", ""),
                    channel_url=ch_data.get("channel_url", ""),
                    avg_views_per_video=ch_data.get("avg_views_per_video", 0),
                    subscribers_per_video=ch_data.get("subscribers_per_video", 0),
                    days_since_creation=ch_data.get("days_since_creation", 0),
                    growth_rate=ch_data.get("growth_rate", "ë³´í†µ"),
                    opportunity_score=ch_data.get("opportunity_score", 0.0),
                    opportunity_label=ch_data.get("opportunity_label", ""),
                    relevance_score=ch_data.get("relevance_score", 0),
                    keyword_relevant=ch_data.get("keyword_relevant", False),
                    relevance_reason=ch_data.get("relevance_reason", "")
                )
                new_channels.append(ch)
            except Exception as e:
                print(f"[ChannelTrend] ì±„ë„ ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        result = TrendAnalysisResult(
            keyword=data["keyword"],
            region=data["region"],
            period_months=data["period_months"],
            analysis_date=data["analysis_date"],
            total_videos_searched=data["total_videos_searched"],
            unique_channels_found=data["unique_channels_found"],
            new_channels_count=data["new_channels_count"],
            new_channels=new_channels,
            monthly_trend=data["monthly_trend"],
            avg_subscribers=data.get("avg_subscribers", 0),
            avg_video_count=data.get("avg_video_count", 0),
            avg_views=data.get("avg_views", 0),
            avg_opportunity_score=data.get("avg_opportunity_score", 0.0),
            market_verdict=data.get("market_verdict", ""),
            market_verdict_label=data.get("market_verdict_label", ""),
            supply_index=data.get("supply_index", 0.0),
            demand_index=data.get("demand_index", 0.0),
            ai_insight=data.get("ai_insight", "")
        )

        # ìºì‹œ ë°ì´í„°ì— ì‹œì¥ ì§€í‘œê°€ ì—†ìœ¼ë©´ ë‹¤ì‹œ ê³„ì‚°
        if not result.market_verdict and result.new_channels:
            result._calculate_market_opportunity()

        return result

    def generate_ai_insight(
        self,
        result: TrendAnalysisResult,
        ai_client=None
    ) -> str:
        """
        AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ìƒì„±

        Args:
            result: íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
            ai_client: AI í´ë¼ì´ì–¸íŠ¸ (Gemini ë˜ëŠ” Claude)
        """
        if not result.new_channels:
            return "ë¶„ì„ ê¸°ê°„ ë‚´ ì‹ ê·œ ì±„ë„ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        channel_summaries = []
        for ch in result.new_channels[:10]:  # ìƒìœ„ 10ê°œë§Œ
            channel_summaries.append(
                f"- {ch.title}: êµ¬ë…ì {ch.subscribers:,}ëª…, "
                f"ì˜ìƒ {ch.video_count}ê°œ, í‰ê· ì¡°íšŒìˆ˜ {ch.avg_views_per_video:,.0f}"
            )

        prompt = f"""
ë‹¤ìŒì€ '{result.keyword}' í‚¤ì›Œë“œë¡œ ìµœê·¼ {result.period_months}ê°œì›” ë‚´ì— ìƒì„±ëœ ì‹ ê·œ YouTube ì±„ë„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.

## ë¶„ì„ ê°œìš”
- ë¶„ì„ ê¸°ê°„: ìµœê·¼ {result.period_months}ê°œì›”
- ê²€ìƒ‰ëœ ì˜ìƒ ìˆ˜: {result.total_videos_searched}ê°œ
- ë°œê²¬ëœ ì‹ ê·œ ì±„ë„: {result.new_channels_count}ê°œ
- í‰ê·  êµ¬ë…ì: {result.avg_subscribers:,.0f}ëª…
- í‰ê·  ì˜ìƒ ìˆ˜: {result.avg_video_count:.1f}ê°œ

## ì£¼ìš” ì‹ ê·œ ì±„ë„
{chr(10).join(channel_summaries)}

## ì›”ë³„ ì±„ë„ ìƒì„± ì¶”ì´
{json.dumps(result.monthly_trend, ensure_ascii=False)}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì´ ë¶„ì•¼ì˜ ì‹œì¥ ì§„ì… ê°•ë„ (ë ˆë“œì˜¤ì…˜/ë¸”ë£¨ì˜¤ì…˜ íŒë‹¨)
2. ì‹ ê·œ ì±„ë„ë“¤ì˜ ê³µí†µì ì¸ íŠ¹ì§•ì´ë‚˜ ì „ëµ
3. ì‹ ê·œ ì§„ì…ìì—ê²Œ ì¶”ì²œí•˜ëŠ” ì°¨ë³„í™” ì „ëµ

3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""

        if ai_client:
            try:
                # AI API í˜¸ì¶œ
                response = ai_client.generate_content(prompt)
                return response.text if hasattr(response, 'text') else str(response)
            except Exception as e:
                print(f"[ChannelTrend] AI ë¶„ì„ ì˜¤ë¥˜: {e}")

        # AI ì—†ì„ ê²½ìš° ê¸°ë³¸ ë¶„ì„
        return self._generate_basic_insight(result)

    def _generate_basic_insight(self, result: TrendAnalysisResult) -> str:
        """ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ìƒì„± (AI ì—†ì„ ë•Œ)"""
        if result.new_channels_count == 0:
            return f"ìµœê·¼ {result.period_months}ê°œì›”ê°„ '{result.keyword}' ë¶„ì•¼ì— ì‹ ê·œ ì±„ë„ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. ë¸”ë£¨ì˜¤ì…˜ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."

        # ì›” í‰ê·  ì‹ ê·œ ì±„ë„ ìˆ˜
        monthly_avg = result.new_channels_count / result.period_months

        # ì„±ì¥ ì±„ë„ ë¹„ìœ¨
        growing = len([c for c in result.new_channels if "ê¸‰ì„±ì¥" in c.growth_rate or "ì–‘í˜¸" in c.growth_rate])
        growing_ratio = growing / result.new_channels_count * 100 if result.new_channels_count > 0 else 0

        insights = []

        # ì§„ì… ê°•ë„ íŒë‹¨
        if monthly_avg >= 5:
            insights.append(f"ğŸ”´ ì›” í‰ê·  {monthly_avg:.1f}ê°œì˜ ì‹ ê·œ ì±„ë„ì´ ìƒì„±ë˜ê³  ìˆì–´ ê²½ìŸì´ ì¹˜ì—´í•œ ë ˆë“œì˜¤ì…˜ì…ë‹ˆë‹¤.")
        elif monthly_avg >= 2:
            insights.append(f"ğŸŸ¡ ì›” í‰ê·  {monthly_avg:.1f}ê°œì˜ ì‹ ê·œ ì±„ë„ì´ ìƒì„±ë˜ë©° ì ë‹¹í•œ ê²½ìŸì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            insights.append(f"ğŸŸ¢ ì›” í‰ê·  {monthly_avg:.1f}ê°œì˜ ì‹ ê·œ ì±„ë„ë§Œ ìƒì„±ë˜ì–´ ì§„ì… ê¸°íšŒê°€ ìˆìŠµë‹ˆë‹¤.")

        # ì„±ì¥ ê°€ëŠ¥ì„±
        if growing_ratio >= 30:
            insights.append(f"ì‹ ê·œ ì±„ë„ ì¤‘ {growing_ratio:.0f}%ê°€ ì–‘í˜¸í•œ ì„±ì¥ì„ ë³´ì—¬ ì‹œì¥ ì ì¬ë ¥ì´ ë†’ìŠµë‹ˆë‹¤.")
        else:
            insights.append(f"ì‹ ê·œ ì±„ë„ ì¤‘ {growing_ratio:.0f}%ë§Œ ì„±ì¥í•˜ê³  ìˆì–´ ì°¨ë³„í™” ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # ì¶”ì²œ
        if result.avg_subscribers > 1000:
            insights.append("í‰ê·  êµ¬ë…ìê°€ ë†’ì•„ ì–‘ì§ˆì˜ ì½˜í…ì¸ ë¡œ ì¶©ë¶„íˆ ì„±ì¥ ê°€ëŠ¥í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.")

        return " ".join(insights)

    def _get_keyword_variants(self, keyword: str) -> List[str]:
        """
        í‚¤ì›Œë“œì˜ ë‹¤ì–‘í•œ ë³€í˜• ìƒì„± (ê´€ë ¨ì„± ê²€ì‚¬ìš©)

        ì˜ˆ: "ë¸Œì´ë¡œê·¸" â†’ ["ë¸Œì´ë¡œê·¸", "vlog", "v-log", "ì¼ìƒ", "ë°ì¼ë¦¬"]
        ì˜ˆ: "ì¼ë³¸" â†’ ["ì¼ë³¸", "japan", "japanese", "ë„ì¿„", "tokyo", ...]
        """
        keyword_lower = keyword.lower()
        variants = [keyword_lower]

        # í•œê¸€-ì˜ì–´ ë§¤í•‘ (í™•ì¥ëœ ë²„ì „)
        korean_english_map = {
            # === êµ­ê°€/ì§€ì—­ ===
            "ì¼ë³¸": ["japan", "japanese", "ì¼ë³¸", "ë„ì¿„", "tokyo", "ì˜¤ì‚¬ì¹´", "osaka",
                    "êµí† ", "kyoto", "í›„ì¿ ì˜¤ì¹´", "fukuoka", "ë‚˜ê³ ì•¼", "nagoya",
                    "ì‚¿í¬ë¡œ", "sapporo", "ì˜¤í‚¤ë‚˜ì™€", "okinawa", "ì¼ë“œ", "j-pop",
                    "jvlog", "ì¼ë³¸ì—¬í–‰", "ì¼ë³¸ìƒí™œ", "ì¼ë³¸ì¼ìƒ", "ì¬íŒ¬", "ë‹ˆí˜¼"],
            "japan": ["japan", "japanese", "ì¼ë³¸", "tokyo", "osaka", "kyoto"],
            "í•œêµ­": ["korea", "korean", "í•œêµ­", "ì„œìš¸", "seoul", "ë¶€ì‚°", "busan",
                    "k-pop", "kpop", "í•œë¥˜", "ì½”ë¦¬ì•„"],
            "ë¯¸êµ­": ["usa", "america", "american", "ë¯¸êµ­", "ë‰´ìš•", "newyork", "la",
                    "los angeles", "ë¯¸êµ­ìƒí™œ", "ë¯¸êµ­ì¼ìƒ"],
            "ì¤‘êµ­": ["china", "chinese", "ì¤‘êµ­", "ë² ì´ì§•", "beijing", "ìƒí•˜ì´", "shanghai"],
            "ìœ ëŸ½": ["europe", "european", "ìœ ëŸ½", "í”„ë‘ìŠ¤", "france", "ë…ì¼", "germany",
                    "ì´íƒˆë¦¬ì•„", "italy", "ìŠ¤í˜ì¸", "spain", "ì˜êµ­", "uk", "london"],
            "ë™ë‚¨ì•„": ["southeast asia", "ë™ë‚¨ì•„", "íƒœêµ­", "thailand", "ë² íŠ¸ë‚¨", "vietnam",
                      "í•„ë¦¬í•€", "philippines", "ì¸ë„ë„¤ì‹œì•„", "indonesia", "ì‹±ê°€í¬ë¥´", "singapore"],

            # === ì½˜í…ì¸  ìœ í˜• ===
            "ë¸Œì´ë¡œê·¸": ["vlog", "v-log", "v log", "ë¸Œì´ë¡œê·¸", "ì¼ìƒ", "ë°ì¼ë¦¬", "daily",
                       "ì¼ìƒê¸°ë¡", "ì¼ìƒë¸Œì´ë¡œê·¸", "vlogger", "ë¸Œì´ë¡œê±°"],
            "vlog": ["vlog", "v-log", "ë¸Œì´ë¡œê·¸", "ì¼ìƒ", "ë°ì¼ë¦¬", "daily", "vlogger"],
            "ë¨¹ë°©": ["mukbang", "eating show", "ë¨¹ë°©", "ë§›ì§‘", "ìŒì‹", "food", "eating",
                    "foodie", "ë¨¹ìŠ¤íƒ€ê·¸ë¨", "í‘¸ë“œ", "ë§›ì§‘íƒë°©", "ë¨¹ë°©ìœ íŠœë²„"],
            "ë¦¬ë·°": ["review", "ë¦¬ë·°", "ì–¸ë°•ì‹±", "unboxing", "ì‚¬ìš©ê¸°", "í›„ê¸°", "ë¦¬ë·°ì–´"],
            "ê²Œì„": ["game", "gaming", "ê²Œì„", "ê²œ", "í”Œë ˆì´", "gameplay", "gamer",
                    "ê²Œì´ë¨¸", "ìŠ¤íŠ¸ë¦¬ë¨¸", "streamer", "ì‹¤í™©", "ê³µëµ"],
            "ì¿ í‚¹": ["cooking", "cook", "ì¿ í‚¹", "ìš”ë¦¬", "ë ˆì‹œí”¼", "recipe", "chef",
                    "ìš”ë¦¬ì‚¬", "ì¿¡ë°©", "ìš”ë¦¬ë²•", "ì§‘ë°¥"],
            "ë·°í‹°": ["beauty", "ë·°í‹°", "ë©”ì´í¬ì—…", "makeup", "í™”ì¥", "ìŠ¤í‚¨ì¼€ì–´", "skincare",
                    "cosmetic", "í™”ì¥í’ˆ", "ë·°í‹°ìœ íŠœë²„"],
            "ì—¬í–‰": ["travel", "ì—¬í–‰", "íŠ¸ë˜ë¸”", "trip", "íˆ¬ì–´", "tour", "ê´€ê´‘", "tourism",
                    "ì—¬í–‰ê¸°", "ì—¬í–‰ë¸Œì´ë¡œê·¸", "traveler", "í•´ì™¸ì—¬í–‰", "êµ­ë‚´ì—¬í–‰"],
            "ì¬í…Œí¬": ["finance", "ì¬í…Œí¬", "íˆ¬ì", "ì£¼ì‹", "ë¶€ë™ì‚°", "money", "stock",
                      "investment", "ë¶€ì—…", "ê²½ì œ", "ê¸ˆìœµ"],
            "ìš´ë™": ["fitness", "workout", "ìš´ë™", "í—¬ìŠ¤", "gym", "ë‹¤ì´ì–´íŠ¸", "diet",
                    "exercise", "í™ˆíŠ¸", "í™ˆíŠ¸ë ˆì´ë‹", "í”¼íŠ¸ë‹ˆìŠ¤"],
            "ê³µë¶€": ["study", "ê³µë¶€", "ìŠ¤í„°ë””", "ê³µìŠ¤íƒ€ê·¸ë¨", "studywithme", "í•™ìŠµ",
                    "ìˆ˜í—˜ìƒ", "ê³µë¶€ë²•", "ìê¸°ê³„ë°œ"],
            "ìœ¡ì•„": ["parenting", "ìœ¡ì•„", "ì•„ê¸°", "baby", "í‚¤ì¦ˆ", "kids", "child",
                    "ì—„ë§ˆ", "ì•„ë¹ ", "ë¶€ëª¨", "ìœ ì•„", "ì–´ë¦°ì´"],
            "ìŒì•…": ["music", "ìŒì•…", "ë…¸ë˜", "song", "singing", "cover", "ì»¤ë²„",
                    "ë®¤ì§", "musician", "ê°€ìˆ˜"],
            "íŒ¨ì…˜": ["fashion", "íŒ¨ì…˜", "ì˜·", "outfit", "ootd", "ìŠ¤íƒ€ì¼", "style",
                    "ì½”ë””", "ì˜ë¥˜", "íŒ¨ì…˜ìœ íŠœë²„"],
            "í…Œí¬": ["tech", "technology", "í…Œí¬", "ê¸°ìˆ ", "it", "gadget", "ê°€ì ¯",
                    "ìŠ¤ë§ˆíŠ¸í°", "ì „ìê¸°ê¸°", "ë¦¬ë·°"],
            "ìë™ì°¨": ["car", "auto", "ìë™ì°¨", "ì°¨ëŸ‰", "ë“œë¼ì´ë¸Œ", "drive", "vehicle",
                      "ì¹´ë¦¬ë·°", "ì‹œìŠ¹ê¸°"],
            "ë°˜ë ¤ë™ë¬¼": ["pet", "ë°˜ë ¤ë™ë¬¼", "ê°•ì•„ì§€", "dog", "ê³ ì–‘ì´", "cat", "ì• ì™„ë™ë¬¼",
                       "í«", "ë™ë¬¼"],
        }

        # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
        if keyword_lower in korean_english_map:
            variants.extend(korean_english_map[keyword_lower])

        # ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œê°€ ë” ê¸´ ê²½ìš° - ì˜ˆ: "ì¼ë³¸ì—¬í–‰")
        for key, values in korean_english_map.items():
            if key in keyword_lower or keyword_lower in key:
                variants.extend(values)

        # ì¤‘ë³µ ì œê±°í•˜ê³  ì›ë³¸ í‚¤ì›Œë“œê°€ í•­ìƒ ì²« ë²ˆì§¸ê°€ ë˜ë„ë¡
        unique_variants = list(set([v.lower() for v in variants]))
        if keyword_lower in unique_variants:
            unique_variants.remove(keyword_lower)
        return [keyword_lower] + unique_variants

    def _calculate_keyword_relevance(
        self,
        title: str,
        description: str,
        keyword_variants: List[str]
    ) -> Tuple[int, bool, str]:
        """
        ì±„ë„ì˜ í‚¤ì›Œë“œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°

        Returns:
            Tuple[int, bool, str]: (ì ìˆ˜ 0-10, ì§ì ‘ê´€ë ¨ì—¬ë¶€, ê´€ë ¨ì„± ì´ìœ )
        """
        score = 0
        reasons = []

        title_lower = title.lower()
        desc_lower = description.lower() if description else ""

        # ì›ë³¸ í‚¤ì›Œë“œ (ì²« ë²ˆì§¸)
        main_keyword = keyword_variants[0] if keyword_variants else ""

        # 1. ì±„ë„ëª…ì— í‚¤ì›Œë“œ ì§ì ‘ í¬í•¨ (ê°€ì¥ ì¤‘ìš”: ìµœëŒ€ +5ì )
        title_match = False
        title_matched_keyword = None
        for variant in keyword_variants:
            if variant in title_lower:
                # ë©”ì¸ í‚¤ì›Œë“œ ë§¤ì¹­ì€ +5ì , ê´€ë ¨ í‚¤ì›Œë“œëŠ” +3ì 
                if variant == main_keyword:
                    score += 5
                    title_matched_keyword = variant
                elif not title_match:  # ì²« ë²ˆì§¸ ê´€ë ¨ í‚¤ì›Œë“œë§Œ
                    score += 3
                    title_matched_keyword = variant
                title_match = True
                reasons.append(f"ì±„ë„ëª…ì— '{variant}' í¬í•¨")
                break

        # 2. ì„¤ëª…ì— í‚¤ì›Œë“œ í¬í•¨ (ìµœëŒ€ +3ì )
        desc_matched = []
        for variant in keyword_variants:
            if variant in desc_lower and variant not in desc_matched:
                desc_matched.append(variant)

        if desc_matched:
            # ë©”ì¸ í‚¤ì›Œë“œê°€ ì„¤ëª…ì— ìˆìœ¼ë©´ +3ì , ê´€ë ¨ í‚¤ì›Œë“œëŠ” ê°œë‹¹ +1ì  (ìµœëŒ€ 2ì )
            if main_keyword in desc_matched:
                score += 3
                reasons.append(f"ì„¤ëª…ì— '{main_keyword}' í¬í•¨")
            else:
                score += min(len(desc_matched), 2)
                reasons.append(f"ì„¤ëª…ì— ê´€ë ¨ì–´ {len(desc_matched)}ê°œ í¬í•¨")

        # 3. í‚¤ì›Œë“œê°€ ì±„ë„ ì£¼ì œì¸ì§€ íŒë‹¨ (í…Œë§ˆ í‚¤ì›Œë“œ íŒ¨í„´)
        theme_keywords = {
            # êµ­ê°€/ì§€ì—­ í…Œë§ˆ
            "ì¼ë³¸": ["ì¼ë³¸ìœ íŠœë²„", "ì¼ë³¸ìƒí™œ", "ì¼ë³¸ë¸Œì´ë¡œê·¸", "ì¬ì¼êµí¬", "ë„ì¿„ìƒí™œ",
                    "ì¼ë³¸ë¨¹ë°©", "ì¼ë³¸ì—¬í–‰", "japan vlog", "ì¼ë³¸ì¼ìƒ", "in japan",
                    "japanese", "tokyo life", "ì¼ë³¸ì´ë¯¼", "ì¼ë³¸ì·¨ì—…"],
            "japan": ["japan vlog", "living in japan", "tokyo", "japanese life"],
            "í•œêµ­": ["í•œêµ­ìœ íŠœë²„", "korean vlog", "korea life", "seoul", "í•œêµ­ìƒí™œ"],
            "ë¯¸êµ­": ["ë¯¸êµ­ìœ íŠœë²„", "ë¯¸êµ­ìƒí™œ", "usa vlog", "american life", "laìƒí™œ"],

            # ì½˜í…ì¸  í…Œë§ˆ
            "ë¸Œì´ë¡œê·¸": ["vlogger", "ì¼ìƒìœ íŠœë²„", "ë°ì¼ë¦¬", "daily life", "ì¼ìƒê¸°ë¡"],
            "vlog": ["vlogger", "daily life", "lifestyle", "day in my life"],
            "ë¨¹ë°©": ["ë¨¹ë°©ëŸ¬", "í‘¸ë“œí¬ë¦¬ì—ì´í„°", "food creator", "eating show", "mukbanger"],
            "ê²Œì„": ["ê²Œì´ë¨¸", "gamer", "streamer", "ê²Œì„ìœ íŠœë²„", "gaming channel"],
            "ë·°í‹°": ["ë·°í‹°í¬ë¦¬ì—ì´í„°", "beauty creator", "makeup artist", "ë·°í‹°ìœ íŠœë²„"],
            "ì—¬í–‰": ["ì—¬í–‰ìœ íŠœë²„", "traveler", "travel vlog", "ì—¬í–‰í¬ë¦¬ì—ì´í„°"],
            "ìŒì•…": ["musician", "singer", "cover artist", "ìŒì•…ìœ íŠœë²„"],
            "ìš”ë¦¬": ["chef", "cook", "cooking channel", "ìš”ë¦¬ìœ íŠœë²„", "ì¿¡ë°©"],
        }

        if main_keyword in theme_keywords:
            for theme in theme_keywords[main_keyword]:
                theme_lower = theme.lower()
                if theme_lower in title_lower or theme_lower in desc_lower:
                    score += 2
                    reasons.append(f"'{theme}' í…Œë§ˆ ë°œê²¬")
                    break

        # 4. ì „ë¬¸ ì±„ë„ íŒ¨í„´ ë³´ë„ˆìŠ¤ (+1ì )
        specialty_patterns = ["ì±„ë„", "channel", "tv", "íŠœë¸Œ", "tube", "ìœ íŠœë²„", "youtuber", "í¬ë¦¬ì—ì´í„°", "creator"]
        for pattern in specialty_patterns:
            if pattern in title_lower:
                # í‚¤ì›Œë“œ + ì±„ë„ëª… íŒ¨í„´ (ì˜ˆ: "ì¼ë³¸ì±„ë„", "japan tv")
                for variant in keyword_variants[:5]:  # ìƒìœ„ 5ê°œ ë³€í˜•ë§Œ í™•ì¸
                    if variant in title_lower:
                        score += 1
                        reasons.append(f"ì „ë¬¸ì±„ë„ íŒ¨í„´")
                        break
                break

        # 5. ê´€ë ¨ ì—†ëŠ” íŒ¨í„´ ê°ì  (ìŠ¤íŒ¸, ìë™ìƒì„± ë“±)
        spam_patterns = ["shorts", "ì‡¼ì¸ ", "í´ë¦½", "clip", "highlight", "í•˜ì´ë¼ì´íŠ¸", "best moments"]
        if title_match is False:  # ì±„ë„ëª…ì— í‚¤ì›Œë“œ ì—†ëŠ” ê²½ìš°ë§Œ ê°ì 
            for pattern in spam_patterns:
                if pattern in title_lower:
                    score = max(0, score - 1)
                    reasons.append(f"'{pattern}' íŒ¨í„´ ê°ì ")
                    break

        # 6. ìµœì¢… ì ìˆ˜ ì •ê·œí™” (0-10)
        final_score = min(10, max(0, score))

        # í‚¤ì›Œë“œ ì§ì ‘ ê´€ë ¨ ì—¬ë¶€ (ì±„ë„ëª…ì— í¬í•¨ë˜ê±°ë‚˜ ì ìˆ˜ 4 ì´ìƒ)
        is_relevant = title_match or final_score >= 4

        reason_str = ", ".join(reasons) if reasons else "ê´€ë ¨ì„± ë‚®ìŒ"

        return final_score, is_relevant, reason_str


# íŒ©í† ë¦¬ í•¨ìˆ˜
def create_channel_trend_analyzer(api_key: str = None) -> ChannelTrendAnalyzer:
    """ì±„ë„ íŠ¸ë Œë“œ ë¶„ì„ê¸° ìƒì„±"""
    return ChannelTrendAnalyzer(api_key=api_key)
