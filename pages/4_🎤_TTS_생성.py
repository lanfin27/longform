# -*- coding: utf-8 -*-
"""
TTS ìƒì„± í˜ì´ì§€

Edge TTSì™€ Chatterbox ì¤‘ ì„ íƒí•˜ì—¬ TTS ìƒì„±
"""
import streamlit as st
import os
import sys
import time
import requests
import tempfile
import io
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# TTS ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from utils.tts_utils import (
    split_text_for_tts,
    get_preview_text,
    validate_chunk_duration,
    merge_chunk_results
)

# ì˜¤ë””ì˜¤ ì •ê·œí™” ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from utils.audio_normalize import (
    normalize_audio_full,
    normalize_scenes_batch,
    normalize_scenes_perfect,
    analyze_audio
)

# ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“ˆ ì„í¬íŠ¸
from utils.chatterbox_async import (
    run_parallel_generation,
    run_threaded_generation
)

# ìˆœì°¨ ì²˜ë¦¬ + ì²­í¬ ë¶„í•  ìœ í‹¸ë¦¬í‹° (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
from utils.chatterbox_utils import (
    generate_scenes_sequential_safe,
    generate_scenes_with_chunking,
    generate_with_chunking
)

# ì •ë°€ ì •ê·œí™” (Â±3% í¸ì°¨ ëª©í‘œ)
from utils.audio_normalize_v2 import (
    AudioNormalizer,
    normalize_scenes_v2,
    analyze_scenes_stats
)

# ê°•ì œ ì •ê·œí™” (ë°˜ë“œì‹œ ì‹¤í–‰, Â±5% í¸ì°¨ ëª©í‘œ, ì†ë„ ë°©í–¥ ìˆ˜ì •ë¨!)
from utils.audio_normalizer_forced import (
    ForcedAudioNormalizer,
    normalize_scenes_forced,
    analyze_normalization_stats
)

# ì™„ë²½ ì •ê·œí™” (3-Pass, Â±1% í¸ì°¨ ëª©í‘œ)
from utils.audio_perfect_normalizer import (
    PerfectAudioNormalizer,
    normalize_perfect
)

# êµ¬ê°„ë³„ ì†ë„ ì •ê·œí™” (ì”¬ ë‚´ ë°œí™”ì†ë„ ê°€ì† ë¬¸ì œ í•´ê²°)
from utils.audio_segment_normalizer import (
    SegmentSpeedNormalizer,
    normalize_segments_all
)

# ë°œí™”ì†ë„ ê°€ì† ë³´ì • (ë’¤ë¡œ ê°ˆìˆ˜ë¡ ë¹¨ë¼ì§€ëŠ” ë¬¸ì œ í•´ê²°)
from utils.audio_speed_corrector import (
    SpeedAccelerationCorrector,
    correct_all_speed_acceleration
)

# â­ í†µí•© ë‹¨ì¼ íŒ¨ìŠ¤ ì²˜ë¦¬ê¸° (FFmpeg 1íšŒë§Œ í˜¸ì¶œ â†’ ìš¸ë¦¼/ë³€ì¡° ë°©ì§€)
from utils.audio_unified_processor import (
    UnifiedAudioProcessor,
    process_all_unified
)

# â­ ì°¸ì¡° ìŒì„± ë¶„ì„ê¸° v2.0 (í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •í™• ì¸¡ì • + íŒŒë¼ë¯¸í„° ìë™ ì¶”ì²œ)
from utils.voice_analyzer import (
    VoiceAnalyzer,
    analyze_voice_and_get_params,
    analyze_voice_with_text,
    get_voice_transcript,
    set_voice_transcript,
    get_profile_manager,
    optimize_voice_for_cloning  # â­ ì°¸ì¡° ìŒì„± ìµœì í™” (15~30ì´ˆ ì¶”ì¶œ)
)

# â­ TTS ìì—°ìŠ¤ëŸ¬ì›€ ìµœì í™” (temperature/repetition_penalty ì¡°ì •)
from utils.tts_naturalness import (
    get_natural_params,
    get_base_natural_params,
    TTSNaturalnessOptimizer
)

# ì§ì ‘ ìƒì„±ê¸° (ì²­í¬ ë¶„í•  ì—†ìŒ - ì†ë„ ìµœì í™”)
from utils.tts_direct_generator import (
    generate_scene_direct,
    generate_all_scenes_direct,
    generate_with_smart_chunking
)

# ë³‘ë ¬ ìƒì„±ê¸° (40% ì†ë„ í–¥ìƒ)
from utils.tts_parallel_generator import (
    generate_scenes_parallel,
    ParallelTTSGenerator
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="TTS ìƒì„±",
    page_icon="ğŸ¤",
    layout="wide"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
.stButton > button { width: 100%; }
.success-box { padding: 1rem; background: #d4edda; border-radius: 0.5rem; }
.error-box { padding: 1rem; background: #f8d7da; border-radius: 0.5rem; }

.voice-card {
    background: white;
    padding: 12px;
    border-radius: 8px;
    border: 2px solid #e0e0e0;
    margin-bottom: 8px;
}

.voice-card:hover {
    border-color: #667eea;
}
</style>
""", unsafe_allow_html=True)

# Chatterbox ì„œë²„ ì„¤ì •
CHATTERBOX_URL = "http://localhost:8100"


# ============================================================
# Edge TTS ìŒì„± ëª©ë¡
# ============================================================

# ì‹¤ì œ Edge TTSì—ì„œ ì§€ì›í•˜ëŠ” ìŒì„±ë§Œ í¬í•¨ (2024ë…„ ê²€ì¦ë¨)
# í•œêµ­ì–´ëŠ” 3ê°œë§Œ ì§€ì›ë¨!
EDGE_VOICES = {
    "ko": [
        {"id": "ko-KR-SunHiNeural", "name": "ì„ í¬", "gender": "ì—¬ì„±", "desc": "ë°ê³  ì¹œê·¼í•¨ (ì¶”ì²œ)"},
        {"id": "ko-KR-InJoonNeural", "name": "ì¸ì¤€", "gender": "ë‚¨ì„±", "desc": "ì°¨ë¶„í•˜ê³  ì‹ ë¢°ê°"},
        {"id": "ko-KR-HyunsuNeural", "name": "í˜„ìˆ˜", "gender": "ë‚¨ì„±", "desc": "ì Šê³  í™œê¸°ì°¸"},
    ],
    "en": [
        {"id": "en-US-JennyNeural", "name": "Jenny", "gender": "ì—¬ì„±", "desc": "ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ì›€ (ì¶”ì²œ)"},
        {"id": "en-US-GuyNeural", "name": "Guy", "gender": "ë‚¨ì„±", "desc": "ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê°"},
        {"id": "en-US-AriaNeural", "name": "Aria", "gender": "ì—¬ì„±", "desc": "ëª…í™•í•˜ê³  í‘œí˜„ë ¥"},
        {"id": "en-US-DavisNeural", "name": "Davis", "gender": "ë‚¨ì„±", "desc": "ê¹Šê³  í’ë¶€í•¨"},
        {"id": "en-GB-SoniaNeural", "name": "Sonia (UK)", "gender": "ì—¬ì„±", "desc": "ì˜êµ­ ì–µì–‘"},
        {"id": "en-GB-RyanNeural", "name": "Ryan (UK)", "gender": "ë‚¨ì„±", "desc": "ì˜êµ­ ì–µì–‘"},
    ],
    "ja": [
        {"id": "ja-JP-NanamiNeural", "name": "ãƒŠãƒŠãƒŸ", "gender": "ì—¬ì„±", "desc": "ë°ê³  ìì—°ìŠ¤ëŸ¬ì›€ (ì¶”ì²œ)"},
        {"id": "ja-JP-KeitaNeural", "name": "ã‚±ã‚¤ã‚¿", "gender": "ë‚¨ì„±", "desc": "ì°¨ë¶„í•˜ê³  ì‹ ë¢°ê°"},
    ],
    "zh": [
        {"id": "zh-CN-XiaoxiaoNeural", "name": "æ™“æ™“", "gender": "ì—¬ì„±", "desc": "ë°ê³  ì¹œê·¼í•¨ (ì¶”ì²œ)"},
        {"id": "zh-CN-YunxiNeural", "name": "äº‘å¸Œ", "gender": "ë‚¨ì„±", "desc": "ì Šê³  í™œê¸°ì°¸"},
        {"id": "zh-CN-YunjianNeural", "name": "äº‘å¥", "gender": "ë‚¨ì„±", "desc": "ê°•í•˜ê³  í˜ìˆìŒ"},
        {"id": "zh-TW-HsiaoChenNeural", "name": "æ›‰è‡» (TW)", "gender": "ì—¬ì„±", "desc": "ëŒ€ë§Œ ì–µì–‘"},
    ]
}

# ê¸°ë³¸ ìŒì„± (í´ë°±ìš©)
DEFAULT_VOICE = {
    "ko": "ko-KR-SunHiNeural",
    "en": "en-US-JennyNeural",
    "ja": "ja-JP-NanamiNeural",
    "zh": "zh-CN-XiaoxiaoNeural",
}


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================

# ì„œë²„ ìƒíƒœ ìºì‹œ TTL (ì´ˆ)
CHATTERBOX_CACHE_TTL = 10


def check_chatterbox_server(force_refresh=False):
    """
    Chatterbox ì„œë²„ ì—°ê²° í™•ì¸ (ìºì‹± ì ìš©)
    - ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (TTL ë‚´)
    - force_refresh=Trueë¡œ ê°•ì œ ìƒˆë¡œê³ ì¹¨ ê°€ëŠ¥
    """
    cache_key = "chatterbox_server_status"
    cache_time_key = "chatterbox_server_status_time"

    # ìºì‹œ í™•ì¸
    if not force_refresh:
        cached_status = st.session_state.get(cache_key)
        cached_time = st.session_state.get(cache_time_key, 0)

        if cached_status is not None and (time.time() - cached_time) < CHATTERBOX_CACHE_TTL:
            return cached_status

    # ì‹¤ì œ ì„œë²„ í™•ì¸
    try:
        r = requests.get(f"{CHATTERBOX_URL}/health", timeout=2)
        status = r.status_code == 200
    except:
        status = False

    # ìºì‹œ ì €ì¥
    st.session_state[cache_key] = status
    st.session_state[cache_time_key] = time.time()

    return status


def get_chatterbox_status(force_refresh=False):
    """
    Chatterbox ì„œë²„ ìƒíƒœ ì¡°íšŒ (ìºì‹± ì ìš©)
    - ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (TTL ë‚´)
    - force_refresh=Trueë¡œ ê°•ì œ ìƒˆë¡œê³ ì¹¨ ê°€ëŠ¥
    """
    cache_key = "chatterbox_model_status"
    cache_time_key = "chatterbox_model_status_time"

    # ìºì‹œ í™•ì¸
    if not force_refresh:
        cached_status = st.session_state.get(cache_key)
        cached_time = st.session_state.get(cache_time_key, 0)

        if cached_status is not None and (time.time() - cached_time) < CHATTERBOX_CACHE_TTL:
            return cached_status

    # ì‹¤ì œ ìƒíƒœ ì¡°íšŒ
    try:
        r = requests.get(f"{CHATTERBOX_URL}/status", timeout=3)
        if r.status_code == 200:
            status = r.json()
        else:
            status = None
    except:
        status = None

    # ìºì‹œ ì €ì¥
    st.session_state[cache_key] = status
    st.session_state[cache_time_key] = time.time()

    return status


def invalidate_chatterbox_cache():
    """Chatterbox ìºì‹œ ë¬´íš¨í™”"""
    for key in ["chatterbox_server_status", "chatterbox_server_status_time",
                "chatterbox_model_status", "chatterbox_model_status_time"]:
        if key in st.session_state:
            del st.session_state[key]


def get_voice_files():
    """ìŒì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒŒì¼ ëª©ë¡"""
    voice_dir = Path("voice_library/ko")
    if voice_dir.exists():
        files = list(voice_dir.glob("*.mp3")) + list(voice_dir.glob("*.wav"))
        return [f.name for f in files]
    return []


# ============================================================
# Chatterbox ì²­í¬ ë¶„í•  ìƒì„± í•¨ìˆ˜
# ============================================================

def render_chatterbox_generation_options():
    """
    Chatterbox ìƒì„± ì˜µì…˜ UI (í”„ë¦¬ë·°/ì „ì²´ ëª¨ë“œ, ì²­í¬ ì„¤ì •)

    Returns:
        dict: ìƒì„± ì˜µì…˜ {mode, preview_length, chunk_size, repetition_penalty}
    """
    st.markdown("#### ğŸ¯ ìƒì„± ëª¨ë“œ")

    col1, col2 = st.columns(2)

    with col1:
        generation_mode = st.radio(
            "ëª¨ë“œ ì„ íƒ",
            ["ğŸ¬ ì „ì²´ ìƒì„± (ê¶Œì¥)", "ğŸ‘ï¸ í”„ë¦¬ë·° (ë¹ ë¥¸ í™•ì¸)"],
            key="chatterbox_generation_mode_option",
            help="""
â€¢ ì „ì²´ ìƒì„±: í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
â€¢ í”„ë¦¬ë·°: ì²˜ìŒ 50ìë§Œ ë¹ ë¥´ê²Œ ìƒì„±í•˜ì—¬ ìŒìƒ‰ í™•ì¸
            """
        )

    preview_length = 50
    with col2:
        if "í”„ë¦¬ë·°" in generation_mode:
            preview_length = st.slider(
                "í”„ë¦¬ë·° ê¸¸ì´ (ê¸€ì)",
                min_value=30,
                max_value=100,
                value=50,
                step=10,
                key="chatter_preview_length"
            )
            st.info(f"ğŸ’¡ ì²˜ìŒ {preview_length}ìë§Œ ìƒì„±í•©ë‹ˆë‹¤.")
        else:
            st.success("âœ… ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    # ê³ ê¸‰ ì˜µì…˜ (ì ‘í˜)
    with st.expander("âš™ï¸ ê³ ê¸‰ ìƒì„± ì˜µì…˜ (í…ìŠ¤íŠ¸ ì˜ë¦¼ ë°©ì§€)"):
        st.caption("ê¸´ í…ìŠ¤íŠ¸ê°€ ì¤‘ê°„ì— ì˜ë¦¬ëŠ” ë¬¸ì œê°€ ìˆë‹¤ë©´ ì•„ë˜ ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”.")

        col_a, col_b = st.columns(2)

        with col_a:
            chunk_size = st.slider(
                "ì²­í¬ í¬ê¸° (ê¸€ì)",
                min_value=40,
                max_value=150,
                value=st.session_state.get("chatter_chunk_size", 80),
                step=10,
                key="chatter_chunk_size_slider",
                help="ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì´ í¬ê¸°ë¡œ ë‚˜ëˆ ì„œ ìƒì„±í•©ë‹ˆë‹¤. ì‘ì„ìˆ˜ë¡ ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦½ë‹ˆë‹¤."
            )
            st.session_state["chatter_chunk_size"] = chunk_size

        with col_b:
            repetition_penalty = st.slider(
                "ë°˜ë³µ ì–µì œ ê°•ë„",
                min_value=1.0,
                max_value=2.0,
                value=st.session_state.get("chatter_rep_penalty", 1.2),  # â­ 1.4â†’1.2 ìì—°ìŠ¤ëŸ¬ì›€ ìµœì í™”
                step=0.1,
                key="chatter_rep_penalty_slider",
                help="ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ì›€. 1.2 ê¶Œì¥ (ê¸°ì¡´ 1.4ëŠ” ë”±ë”±í•¨)"
            )
            st.session_state["chatter_rep_penalty"] = repetition_penalty

        col_c, col_d = st.columns(2)

        with col_c:
            max_retries = st.number_input(
                "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜",
                min_value=1,
                max_value=5,
                value=st.session_state.get("chatter_max_retries", 3),
                key="chatter_max_retries_input",
                help="ì˜ë¦¼ ê°ì§€ ì‹œ ì¬ì‹œë„ íšŸìˆ˜"
            )
            st.session_state["chatter_max_retries"] = max_retries

        with col_d:
            pause_ms = st.slider(
                "ì²­í¬ ê°„ íœ´ì‹ (ms)",
                min_value=0,
                max_value=500,
                value=st.session_state.get("chatter_pause_ms", 200),
                step=50,
                key="chatter_pause_ms_slider",
                help="ì²­í¬ ì‚¬ì´ì— ì‚½ì…í•  ë¬´ìŒ ê¸¸ì´"
            )
            st.session_state["chatter_pause_ms"] = pause_ms

        # ì²˜ë¦¬ ë°©ì‹ ì˜µì…˜
        st.markdown("---")
        st.markdown("**âš¡ ì²˜ë¦¬ ë°©ì‹ ì„¤ì •**")

        col_e, col_f = st.columns(2)

        with col_e:
            processing_mode = st.radio(
                "ì²˜ë¦¬ ë°©ì‹",
                ["ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ (ì•ˆì •ì )", "ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ (ë¹ ë¦„)"],
                index=0,  # ê¸°ë³¸: ìˆœì°¨ ì²˜ë¦¬
                key="chatter_processing_mode",
                help="""
â€¢ ìˆœì°¨ ì²˜ë¦¬: íƒ€ì„ì•„ì›ƒ ë°©ì§€, ì•ˆì •ì  (ê¶Œì¥)
â€¢ ë³‘ë ¬ ì²˜ë¦¬: ë¹ ë¥´ì§€ë§Œ GPU ê²½ìŸìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ê°€ëŠ¥
                """
            )
            use_sequential = "ìˆœì°¨" in processing_mode

            # ìˆœì°¨ ì²˜ë¦¬ ì‹œ ì²­í¬ ë¶„í•  ì˜µì…˜
            if use_sequential:
                use_smart_chunking = st.checkbox(
                    "ğŸ“ ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• ",
                    value=st.session_state.get("chatter_smart_chunking", True),
                    key="chatter_smart_chunking_checkbox",
                    help="ê¸´ í…ìŠ¤íŠ¸ë¥¼ 70ì ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."
                )
                st.session_state["chatter_smart_chunking"] = use_smart_chunking
            else:
                use_smart_chunking = False

        with col_f:
            if use_sequential:
                timeout_per_scene = st.slider(
                    "ì”¬ë‹¹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)",
                    min_value=60,
                    max_value=300,
                    value=st.session_state.get("chatter_timeout", 180),
                    step=30,
                    key="chatter_timeout_slider",
                    help="ê¸´ í…ìŠ¤íŠ¸ëŠ” ë” ê¸´ íƒ€ì„ì•„ì›ƒì´ í•„ìš”í•©ë‹ˆë‹¤. (ê¸°ë³¸: 180ì´ˆ)"
                )
                st.session_state["chatter_timeout"] = timeout_per_scene
                max_concurrent = 1
            else:
                max_concurrent = st.slider(
                    "ë™ì‹œ ìƒì„± ìˆ˜",
                    min_value=1,
                    max_value=4,
                    value=st.session_state.get("chatter_max_concurrent", 2),
                    step=1,
                    key="chatter_max_concurrent_slider",
                    help="ë™ì‹œì— ìƒì„±í•  ì”¬ ìˆ˜. (ê¸°ë³¸: 2)"
                )
                st.session_state["chatter_max_concurrent"] = max_concurrent
                timeout_per_scene = 180
                use_smart_chunking = False

    return {
        "mode": "preview" if "í”„ë¦¬ë·°" in generation_mode else "full",
        "preview_length": preview_length,
        "chunk_size": chunk_size,
        "repetition_penalty": repetition_penalty,
        "max_retries": max_retries,
        "pause_ms": pause_ms,
        "use_sequential": use_sequential,
        "use_smart_chunking": use_smart_chunking,
        "timeout_per_scene": timeout_per_scene,
        "parallel_enabled": not use_sequential,
        "max_concurrent": max_concurrent
    }


def render_normalization_options():
    """
    ìŒì„± ì •ê·œí™” ì˜µì…˜ UI

    Returns:
        dict: ì •ê·œí™” ì˜µì…˜ {enabled, target_lufs, normalize_speed, normalize_silence}
    """
    st.markdown("#### ğŸšï¸ ìŒì„± ì¼ê´€ì„± ì„¤ì •")

    with st.expander("ìŒì„± ì •ê·œí™” ì˜µì…˜", expanded=False):
        st.caption("ì”¬ë³„ ìŒëŸ‰, ì†ë„, ë¬´ìŒ êµ¬ê°„ì„ ì¼ê´€ë˜ê²Œ ë§ì¶¥ë‹ˆë‹¤.")

        col1, col2 = st.columns(2)

        with col1:
            enable_normalization = st.checkbox(
                "âœ… ìŒì„± ì •ê·œí™” ì ìš©",
                value=st.session_state.get("enable_normalization", True),
                key="enable_norm_checkbox",
                help="ì”¬ë³„ ìŒëŸ‰, ì†ë„ë¥¼ ì¼ê´€ë˜ê²Œ ë§ì¶¥ë‹ˆë‹¤."
            )
            st.session_state["enable_normalization"] = enable_normalization

            if enable_normalization:
                target_lufs = st.slider(
                    "ğŸ”Š ëª©í‘œ ìŒëŸ‰ (LUFS)",
                    min_value=-24,
                    max_value=-12,
                    value=st.session_state.get("target_lufs", -16),
                    step=1,
                    key="target_lufs_slider",
                    help="-16 LUFS: ìŠ¤íŠ¸ë¦¬ë° í‘œì¤€\n-14 LUFS: ì•½ê°„ í° ì†Œë¦¬\n-20 LUFS: ì¡°ìš©í•œ ì†Œë¦¬"
                )
                st.session_state["target_lufs"] = target_lufs
            else:
                target_lufs = -16

        with col2:
            if enable_normalization:
                normalize_speed = st.checkbox(
                    "â±ï¸ ë°œí™” ì†ë„ ì¼ê´€ì„±",
                    value=st.session_state.get("normalize_speed", True),
                    key="normalize_speed_checkbox",
                    help="ëª¨ë“  ì”¬ì˜ ë°œí™” ì†ë„ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ë§ì¶¥ë‹ˆë‹¤."
                )
                st.session_state["normalize_speed"] = normalize_speed

                normalize_silence = st.checkbox(
                    "ğŸ”‡ ë¬´ìŒ êµ¬ê°„ í‘œì¤€í™”",
                    value=st.session_state.get("normalize_silence", True),
                    key="normalize_silence_checkbox",
                    help="ê° ì”¬ ì•ë’¤ì˜ ë¬´ìŒ êµ¬ê°„ì„ 100msë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤."
                )
                st.session_state["normalize_silence"] = normalize_silence
            else:
                normalize_speed = False
                normalize_silence = False

    return {
        "enabled": enable_normalization,
        "target_lufs": target_lufs,
        "normalize_speed": normalize_speed,
        "normalize_silence": normalize_silence
    }


def apply_normalization_to_result(
    result: dict,
    text: str,
    norm_opts: dict
) -> dict:
    """
    ë‹¨ì¼ TTS ê²°ê³¼ì— ì •ê·œí™” ì ìš©

    Args:
        result: TTS ìƒì„± ê²°ê³¼ (audio_data í¬í•¨)
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        norm_opts: ì •ê·œí™” ì˜µì…˜

    Returns:
        ì •ê·œí™”ëœ ê²°ê³¼ dict
    """
    print(f"[Normalize] apply_normalization_to_result í˜¸ì¶œ")
    print(f"[Normalize] ì˜µì…˜: enabled={norm_opts.get('enabled')}, target_lufs={norm_opts.get('target_lufs')}")

    if not norm_opts.get("enabled", True):
        print("[Normalize] âŒ ì •ê·œí™” ë¹„í™œì„±í™”ë¨ - ìŠ¤í‚µ")
        return result

    if not result.get("success") or not result.get("audio_data"):
        print("[Normalize] âŒ ìœ íš¨í•œ ì˜¤ë””ì˜¤ ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        return result

    try:
        print(f"[Normalize] ì •ê·œí™” ì‹œì‘: í…ìŠ¤íŠ¸ {len(text)}ì")
        normalized = normalize_audio_full(
            audio_data=result["audio_data"],
            text=text,
            target_lufs=norm_opts.get("target_lufs", -16),
            target_speech_rate=None,  # ë‹¨ì¼ ìƒì„±ì—ì„œëŠ” ì†ë„ ì¡°ì • ì•ˆí•¨
            standardize_silence_ms=(100, 100) if norm_opts.get("normalize_silence", True) else None
        )

        result["audio_data"] = normalized["audio_data"]
        result["original_rate"] = normalized["original_rate"]
        result["final_rate"] = normalized["final_rate"]
        result["original_duration"] = normalized["original_duration"]
        result["final_duration"] = normalized["final_duration"]
        result["normalized"] = True

        print(f"[Normalize] âœ… ì •ê·œí™” ì™„ë£Œ: {normalized['original_duration']:.1f}ì´ˆ â†’ {normalized['final_duration']:.1f}ì´ˆ")

    except Exception as e:
        print(f"[Normalize] âŒ ì •ê·œí™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        result["normalized"] = False

    return result


def apply_normalization_to_scenes(
    scene_results: list,
    norm_opts: dict,
    progress_callback=None
) -> list:
    """
    ì”¬ë³„ TTS ê²°ê³¼ì— ì¼ê´„ ì •ê·œí™” ì ìš©

    Args:
        scene_results: ì”¬ë³„ ìƒì„± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        norm_opts: ì •ê·œí™” ì˜µì…˜
        progress_callback: ì§„í–‰ ì½œë°±

    Returns:
        ì •ê·œí™”ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    print(f"[Normalization] apply_normalization_to_scenes í˜¸ì¶œ")
    print(f"[Normalization] ì˜µì…˜: enabled={norm_opts.get('enabled')}, target_lufs={norm_opts.get('target_lufs')}")

    if not norm_opts.get("enabled", True):
        print("[Normalization] âŒ ì •ê·œí™” ë¹„í™œì„±í™”ë¨ - ìŠ¤í‚µ")
        return scene_results

    # ì„±ê³µí•œ ì”¬ë§Œ í•„í„°ë§
    valid_scenes = []
    for r in scene_results:
        if r.get("audio_data"):
            valid_scenes.append({
                "scene_id": r.get("scene_id"),
                "audio_data": r.get("audio_data"),
                "text": r.get("text_preview", "") if len(r.get("text_preview", "")) > 30 else r.get("text", "")
            })

    print(f"[Normalization] ìœ íš¨í•œ ì”¬ ìˆ˜: {len(valid_scenes)}/{len(scene_results)}")

    if not valid_scenes:
        print("[Normalization] âŒ ìœ íš¨í•œ ì”¬ ì—†ìŒ - ìŠ¤í‚µ")
        return scene_results

    # ë°°ì¹˜ ì •ê·œí™” ì ìš©
    print(f"[Normalization] ë°°ì¹˜ ì •ê·œí™” ì‹œì‘...")
    normalized = normalize_scenes_batch(
        scene_audios=valid_scenes,
        target_lufs=norm_opts.get("target_lufs", -16),
        use_consistent_speed=norm_opts.get("normalize_speed", True),
        standardize_silence_ms=(100, 100) if norm_opts.get("normalize_silence", True) else None,
        progress_callback=progress_callback
    )
    print(f"[Normalization] âœ… ë°°ì¹˜ ì •ê·œí™” ì™„ë£Œ: {len(normalized)}ê°œ ì”¬")

    # ê²°ê³¼ ë³‘í•©
    normalized_map = {n["scene_id"]: n for n in normalized}

    normalized_count = 0
    for r in scene_results:
        scene_id = r.get("scene_id")
        if scene_id in normalized_map:
            norm_data = normalized_map[scene_id]
            r["audio_data"] = norm_data.get("audio_data", r.get("audio_data"))
            r["original_rate"] = norm_data.get("original_rate", 0)
            r["final_rate"] = norm_data.get("final_rate", 0)
            r["original_duration"] = norm_data.get("original_duration", 0)
            r["final_duration"] = norm_data.get("final_duration", 0)
            r["normalized"] = norm_data.get("normalized", False)
            if r["normalized"]:
                normalized_count += 1

    print(f"[Normalization] ìµœì¢… ê²°ê³¼: {normalized_count}ê°œ ì”¬ ì •ê·œí™”ë¨")
    return scene_results


def generate_single_chunk(
    text: str,
    voice_ref_path: str,
    params: dict,
    repetition_penalty: float = 1.3,
    timeout: int = 120
) -> dict:
    """
    ë‹¨ì¼ ì²­í¬ TTS ìƒì„±

    Args:
        text: í…ìŠ¤íŠ¸
        voice_ref_path: ì°¸ì¡° ìŒì„± ê²½ë¡œ
        params: TTS íŒŒë¼ë¯¸í„°
        repetition_penalty: ë°˜ë³µ ì–µì œ ê°•ë„
        timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    Returns:
        {success, audio_data, duration, error}
    """
    # ë””ë²„ê·¸ ë¡œê¹…
    seed_value = params.get("seed")
    print(f"[TTS] generate_single_chunk í˜¸ì¶œ:")
    print(f"  - text: {text[:30]}...")
    print(f"  - voice_ref_path: {voice_ref_path}")
    print(f"  - seed: {seed_value} ({'ê³ ì •' if seed_value is not None else 'ëœë¤'})")

    payload = {
        "text": text,
        "settings": {
            "language": "ko",
            "exaggeration": params.get("exaggeration", 0.5),
            "cfg_weight": params.get("cfg_weight", 0.5),
            "temperature": params.get("temperature", 0.8),
            "speed": params.get("speed", 1.0),
            "seed": seed_value,
            "voice_ref_path": voice_ref_path,
            "repetition_penalty": repetition_penalty
        }
    }

    try:
        start_time = time.time()
        r = requests.post(f"{CHATTERBOX_URL}/generate", json=payload, timeout=timeout)
        elapsed = time.time() - start_time

        if r.status_code == 200:
            result = r.json()

            if result.get("success"):
                # ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
                audio_url = result.get("audio_url", "")
                audio_data = None

                if audio_url:
                    full_url = f"{CHATTERBOX_URL}{audio_url}"
                    audio_response = requests.get(full_url, timeout=30)
                    if audio_response.status_code == 200:
                        audio_data = audio_response.content

                return {
                    "success": True,
                    "audio_data": audio_data,
                    "duration": result.get("duration_seconds", 0),
                    "processing_time": elapsed,
                    "seed_used": result.get("seed_used")
                }
            else:
                return {
                    "success": False,
                    "error": result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                }
        else:
            return {
                "success": False,
                "error": f"HTTP {r.status_code}"
            }

    except requests.exceptions.Timeout:
        return {"success": False, "error": "íƒ€ì„ì•„ì›ƒ"}
    except requests.exceptions.ConnectionError:
        invalidate_chatterbox_cache()
        return {"success": False, "error": "ì„œë²„ ì—°ê²° ì‹¤íŒ¨"}
    except Exception as e:
        return {"success": False, "error": str(e)}


def generate_chunk_with_retry(
    text: str,
    voice_ref_path: str,
    params: dict,
    initial_rep_penalty: float = 1.3,
    max_retries: int = 3
) -> dict:
    """
    ì²­í¬ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

    ì˜ë¦¼ ê°ì§€ ì‹œ seed ë³€ê²½ + repetition_penalty ì¦ê°€ í›„ ì¬ì‹œë„.

    Args:
        text: í…ìŠ¤íŠ¸
        voice_ref_path: ì°¸ì¡° ìŒì„± ê²½ë¡œ
        params: TTS íŒŒë¼ë¯¸í„°
        initial_rep_penalty: ì´ˆê¸° ë°˜ë³µ ì–µì œ ê°•ë„
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

    Returns:
        ìƒì„± ê²°ê³¼ dict
    """
    import random
    current_rep_penalty = initial_rep_penalty
    text_length = len(text)
    base_seed = params.get("seed")

    for attempt in range(max_retries):
        # ì¬ì‹œë„ ì‹œ seed ë³€ê²½ (ë‹¤ë¥¸ í† í° íŒ¨í„´ ìœ ë„)
        retry_params = params.copy()
        if attempt > 0:
            if base_seed is not None:
                retry_params["seed"] = base_seed + (attempt * 1000)
            else:
                retry_params["seed"] = random.randint(0, 2**31 - 1)
            print(f"[Retry {attempt}] seed={retry_params['seed']}, rep_penalty={current_rep_penalty}")

        result = generate_single_chunk(
            text=text,
            voice_ref_path=voice_ref_path,
            params=retry_params,
            repetition_penalty=current_rep_penalty
        )

        if not result.get("success"):
            # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
            if attempt < max_retries - 1:
                current_rep_penalty += 0.2
                time.sleep(0.5)
                continue
            return result

        # ì˜ë¦¼ ê²€ì¦
        duration = result.get("duration", 0)
        validation = validate_chunk_duration(text_length, duration)

        if validation["is_valid"]:
            result["status"] = "success"
            result["char_count"] = text_length
            chars_per_sec = validation.get("chars_per_second", 0)
            print(f"[Generate] âœ… ì •ìƒ: {duration:.2f}ì´ˆ, {chars_per_sec:.1f} ê¸€ì/ì´ˆ")
            return result
        else:
            # ì˜ë¦¼ ê°ì§€!
            chars_per_sec = validation.get("chars_per_second", 0)
            print(f"[Generate] âš ï¸ ì˜ë¦¼: {duration:.2f}ì´ˆ < {validation['expected_min']:.2f}ì´ˆ ({chars_per_sec:.1f} ê¸€ì/ì´ˆ)")

            if attempt < max_retries - 1:
                # ì¬ì‹œë„: seed ë³€ê²½ + rep_penalty ì¦ê°€
                current_rep_penalty = min(current_rep_penalty + 0.3, 2.5)
                time.sleep(0.3)
                continue
            else:
                # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë„ ì˜ë¦¼ â†’ ê·¸ë˜ë„ ë°˜í™˜
                result["status"] = "truncated"
                result["char_count"] = text_length
                result["warning"] = f"í…ìŠ¤íŠ¸ ì˜ë¦¼ ({chars_per_sec:.1f} ê¸€ì/ì´ˆ, ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼)"
                return result

    return {"success": False, "error": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼"}


def merge_audio_chunks(audio_data_list: list, pause_ms: int = 200) -> bytes:
    """
    ì˜¤ë””ì˜¤ ì²­í¬ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©

    Args:
        audio_data_list: ì˜¤ë””ì˜¤ ë°ì´í„° ë°”ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
        pause_ms: ì²­í¬ ê°„ íœ´ì‹ ì‹œê°„ (ms)

    Returns:
        ë³‘í•©ëœ WAV ë°”ì´íŠ¸
    """
    try:
        from pydub import AudioSegment

        combined = AudioSegment.empty()
        silence = AudioSegment.silent(duration=pause_ms) if pause_ms > 0 else None

        for idx, audio_data in enumerate(audio_data_list):
            if not audio_data:
                continue

            # BytesIOë¡œ ë³€í™˜í•˜ì—¬ ë¡œë“œ
            audio_io = io.BytesIO(audio_data)
            try:
                audio = AudioSegment.from_file(audio_io, format="wav")

                if idx > 0 and silence:
                    combined += silence

                combined += audio
            except Exception as e:
                print(f"[Merge] ì²­í¬ {idx + 1} ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue

        # ê²°ê³¼ë¥¼ BytesIOë¡œ ì¶œë ¥
        output_io = io.BytesIO()
        combined.export(output_io, format="wav")
        output_io.seek(0)

        return output_io.read()

    except ImportError:
        # pydubì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì²­í¬ë§Œ ë°˜í™˜
        for audio_data in audio_data_list:
            if audio_data:
                return audio_data
        return b""
    except Exception as e:
        print(f"[Merge] ë³‘í•© ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ì²« ë²ˆì§¸ ì²­í¬ë§Œ ë°˜í™˜
        for audio_data in audio_data_list:
            if audio_data:
                return audio_data
        return b""


def generate_chatterbox_tts_robust(
    text: str,
    voice_ref_path: str,
    params: dict,
    mode: str = "full",
    preview_length: int = 50,
    chunk_size: int = 80,
    repetition_penalty: float = 1.3,
    max_retries: int = 3,
    pause_ms: int = 200,
    progress_callback=None
) -> dict:
    """
    ì•ˆì •ì ì¸ Chatterbox TTS ìƒì„± (ì²­í¬ ë¶„í•  + ì¬ì‹œë„)

    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ìƒì„±í•˜ê³  ë³‘í•©í•©ë‹ˆë‹¤.
    í† í° ë°˜ë³µìœ¼ë¡œ ì¸í•œ ì¡°ê¸° ì¢…ë£Œë¥¼ ê°ì§€í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.

    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        voice_ref_path: ì°¸ì¡° ìŒì„± íŒŒì¼ ê²½ë¡œ
        params: TTS íŒŒë¼ë¯¸í„°
        mode: "full" (ì „ì²´) ë˜ëŠ” "preview" (í”„ë¦¬ë·°)
        preview_length: í”„ë¦¬ë·° ëª¨ë“œ ì‹œ ê¸€ì ìˆ˜
        chunk_size: ì²­í¬ë‹¹ ìµœëŒ€ ê¸€ì ìˆ˜
        repetition_penalty: ë°˜ë³µ ì–µì œ ê°•ë„
        max_retries: ì²­í¬ë‹¹ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        pause_ms: ì²­í¬ ê°„ íœ´ì‹ ì‹œê°„ (ms)
        progress_callback: ì§„í–‰ ìƒí™© ì½œë°± (current, total, message)

    Returns:
        {success, audio_data, duration, chunks_info, ...}
    """
    # 1. í”„ë¦¬ë·° ëª¨ë“œë©´ í…ìŠ¤íŠ¸ ìë¥´ê¸°
    original_length = len(text)
    if mode == "preview":
        text = get_preview_text(text, preview_length)
        if progress_callback:
            progress_callback(0, 1, f"í”„ë¦¬ë·° ëª¨ë“œ: {len(text)}ì ìƒì„±")

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    chunks = split_text_for_tts(text, max_chars=chunk_size)
    total_chunks = len(chunks)

    if progress_callback:
        progress_callback(0, total_chunks, f"ì´ {total_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• ")

    # 3. ê° ì²­í¬ ìƒì„±
    audio_data_list = []
    chunks_info = []

    for chunk_data in chunks:
        chunk_idx = chunk_data["index"]
        chunk_text = chunk_data["text"]

        if progress_callback:
            progress_callback(chunk_idx - 1, total_chunks, f"ì²­í¬ {chunk_idx}/{total_chunks} ìƒì„± ì¤‘...")

        # ì²­í¬ ìƒì„± (ì¬ì‹œë„ í¬í•¨)
        result = generate_chunk_with_retry(
            text=chunk_text,
            voice_ref_path=voice_ref_path,
            params=params,
            initial_rep_penalty=repetition_penalty,
            max_retries=max_retries
        )

        if result.get("success") or result.get("status") == "truncated":
            audio_data_list.append(result.get("audio_data"))
            chunks_info.append({
                "index": chunk_idx,
                "text_preview": chunk_text[:30] + "..." if len(chunk_text) > 30 else chunk_text,
                "char_count": len(chunk_text),
                "duration": result.get("duration", 0),
                "status": result.get("status", "success"),
                "warning": result.get("warning")
            })
        else:
            chunks_info.append({
                "index": chunk_idx,
                "text_preview": chunk_text[:30] + "...",
                "char_count": len(chunk_text),
                "error": result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"),
                "status": "failed"
            })

    if progress_callback:
        progress_callback(total_chunks, total_chunks, "ì˜¤ë””ì˜¤ ë³‘í•© ì¤‘...")

    # 4. ê²°ê³¼ ì§‘ê³„
    stats = merge_chunk_results(chunks_info)

    if stats["success_count"] == 0:
        return {
            "success": False,
            "error": "ëª¨ë“  ì²­í¬ ìƒì„± ì‹¤íŒ¨",
            "chunks_info": chunks_info,
            "stats": stats
        }

    # 5. ì˜¤ë””ì˜¤ ë³‘í•©
    valid_audio_data = [a for a in audio_data_list if a]

    if len(valid_audio_data) == 1:
        # ë‹¨ì¼ ì²­í¬ë©´ ë³‘í•© ë¶ˆí•„ìš”
        merged_audio = valid_audio_data[0]
    else:
        merged_audio = merge_audio_chunks(valid_audio_data, pause_ms=pause_ms)

    return {
        "success": True,
        "audio_data": merged_audio,
        "duration": stats["total_duration"],
        "chunks_info": chunks_info,
        "stats": stats,
        "mode": mode,
        "original_length": original_length,
        "processed_length": len(text)
    }


# ============================================================
# ì„¤ì • íƒ­
# ============================================================

def render_settings_tab():
    """ê³µí†µ ì„¤ì • íƒ­"""
    st.markdown("### âš™ï¸ TTS ê³µí†µ ì„¤ì •")

    # ì–¸ì–´ ì„ íƒ
    st.markdown("#### ğŸŒ ì–¸ì–´ ì„ íƒ")

    lang_cols = st.columns(4)
    languages = [
        ("ko", "ğŸ‡°ğŸ‡· í•œêµ­ì–´"),
        ("en", "ğŸ‡ºğŸ‡¸ ì˜ì–´"),
        ("ja", "ğŸ‡¯ğŸ‡µ ì¼ë³¸ì–´"),
        ("zh", "ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´"),
    ]

    selected_lang = st.session_state.get("tts_lang", "ko")

    for i, (code, name) in enumerate(languages):
        with lang_cols[i]:
            if st.button(
                name,
                key=f"setting_lang_{code}",
                type="primary" if selected_lang == code else "secondary",
                use_container_width=True
            ):
                st.session_state["tts_lang"] = code
                st.session_state["edge_tts_lang"] = code
                st.rerun()

    st.info(f"ì„ íƒëœ ì–¸ì–´: **{dict(languages).get(selected_lang, selected_lang)}**")

    st.markdown("---")

    # ì‹œë‹ˆì–´ ì¹œí™” ì„¤ì •
    st.markdown("#### ğŸ‘´ ì‹œë‹ˆì–´ ì¹œí™” ì„¤ì •")

    col1, col2 = st.columns(2)

    with col1:
        add_breaks = st.checkbox(
            "ë¬¸ë‹¨ ì‚¬ì´ ë¬´ìŒ ì¶”ê°€",
            value=st.session_state.get("setting_breaks", True),
            key="setting_breaks_cb"
        )
        st.session_state["setting_breaks"] = add_breaks

        if add_breaks:
            break_length = st.slider(
                "ë¬´ìŒ ê¸¸ì´ (ì´ˆ)",
                min_value=0.5,
                max_value=3.0,
                value=st.session_state.get("setting_break_length", 1.5),
                step=0.5,
                key="setting_break_length_slider"
            )
            st.session_state["setting_break_length"] = break_length
            st.caption(f"ë¬¸ë‹¨ ì‚¬ì´ì— {break_length}ì´ˆ ë¬´ìŒì´ ì‚½ì…ë©ë‹ˆë‹¤.")

    with col2:
        slow_mode = st.checkbox(
            "ëŠë¦° ì†ë„ ëª¨ë“œ (ì‹œë‹ˆì–´ìš©)",
            value=st.session_state.get("setting_slow", False),
            key="setting_slow_cb"
        )
        st.session_state["setting_slow"] = slow_mode

        if slow_mode:
            st.session_state["edge_tts_rate"] = -20
            st.caption("ê¸°ë³¸ ì†ë„ê°€ -20%ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.")

    st.markdown("---")

    # ì¶œë ¥ ì„¤ì •
    st.markdown("#### ğŸ“ ì¶œë ¥ ì„¤ì •")

    output_dir = st.text_input(
        "ì¶œë ¥ ë””ë ‰í† ë¦¬",
        value=st.session_state.get("tts_output_dir", "data/tts"),
        key="tts_output_dir_input"
    )
    st.session_state["tts_output_dir"] = output_dir

    generate_srt = st.checkbox(
        "ìë§‰ íŒŒì¼ ìë™ ìƒì„± (SRT)",
        value=st.session_state.get("auto_srt", True),
        key="auto_srt_cb"
    )
    st.session_state["auto_srt"] = generate_srt


# ============================================================
# Edge TTS íƒ­
# ============================================================

def render_edge_tts_tab():
    """Edge TTS íƒ­ ë Œë”ë§"""
    st.markdown("### âœ¨ Edge TTS")
    st.info("Microsoftì˜ ë¬´ë£Œ TTS ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ìŒì„±ê³¼ ì–¸ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.")

    # ìŒì„± í˜„í™© í‘œì‹œ
    with st.expander("ğŸ“‹ ì§€ì› ìŒì„± í˜„í™©", expanded=False):
        st.markdown("""
        | ì–¸ì–´ | ì§€ì› ìŒì„± |
        |------|----------|
        | ğŸ‡°ğŸ‡· í•œêµ­ì–´ | ì„ í¬(ì—¬), ì¸ì¤€(ë‚¨), í˜„ìˆ˜(ë‚¨) - **3ê°œ** |
        | ğŸ‡ºğŸ‡¸ ì˜ì–´ | Jenny, Guy, Aria, Davis, Sonia(UK), Ryan(UK) - **6ê°œ** |
        | ğŸ‡¯ğŸ‡µ ì¼ë³¸ì–´ | ãƒŠãƒŠãƒŸ(ì—¬), ã‚±ã‚¤ã‚¿(ë‚¨) - **2ê°œ** |
        | ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´ | æ™“æ™“, äº‘å¸Œ, äº‘å¥, æ›‰è‡»(TW) - **4ê°œ** |
        """)

    # === ì–¸ì–´ ì„ íƒ ===
    st.markdown("#### ğŸŒ ì–¸ì–´ ì„ íƒ")

    lang_cols = st.columns(4)
    languages = [("ko", "ğŸ‡°ğŸ‡· í•œêµ­ì–´"), ("en", "ğŸ‡ºğŸ‡¸ ì˜ì–´"), ("ja", "ğŸ‡¯ğŸ‡µ ì¼ë³¸ì–´"), ("zh", "ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´")]

    selected_lang = st.session_state.get("edge_tts_lang", "ko")

    for i, (code, name) in enumerate(languages):
        with lang_cols[i]:
            if st.button(
                name,
                key=f"edge_lang_{code}",
                type="primary" if selected_lang == code else "secondary",
                use_container_width=True
            ):
                st.session_state["edge_tts_lang"] = code
                st.rerun()

    selected_lang = st.session_state.get("edge_tts_lang", "ko")

    st.markdown("---")

    # === ìŒì„± ì„ íƒ ===
    st.markdown("#### ğŸ™ï¸ ìŒì„± ì„ íƒ")

    voices = EDGE_VOICES.get(selected_lang, EDGE_VOICES["ko"])
    all_voice_ids = [v["id"] for v in voices]

    # ì €ì¥ëœ ìŒì„±ì´ í˜„ì¬ ì–¸ì–´ì—ì„œ ìœ íš¨í•œì§€ í™•ì¸
    stored_voice = st.session_state.get("selected_edge_voice", "")
    if stored_voice and stored_voice not in all_voice_ids:
        # ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ìŒì„±ìœ¼ë¡œ ì¬ì„¤ì •
        st.session_state["selected_edge_voice"] = DEFAULT_VOICE.get(selected_lang, voices[0]["id"])

    # ì„±ë³„ í•„í„°
    gender_filter = st.radio(
        "ì„±ë³„ í•„í„°",
        options=["ì „ì²´", "ì—¬ì„±", "ë‚¨ì„±"],
        horizontal=True,
        key="edge_gender_filter"
    )

    filtered_voices = voices
    if gender_filter != "ì „ì²´":
        filtered_voices = [v for v in voices if v["gender"] == gender_filter]

    # í•„í„°ë§ í›„ ìŒì„±ì´ ì—†ìœ¼ë©´ ì „ì²´ ìŒì„± í‘œì‹œ
    if not filtered_voices:
        filtered_voices = voices

    # ìŒì„± ê·¸ë¦¬ë“œ
    selected_voice = st.session_state.get("selected_edge_voice", DEFAULT_VOICE.get(selected_lang, voices[0]["id"]))

    cols = st.columns(3)
    for i, voice in enumerate(filtered_voices):
        with cols[i % 3]:
            is_selected = voice["id"] == selected_voice
            icon = "ğŸ‘©" if voice["gender"] == "ì—¬ì„±" else "ğŸ‘¨"

            if st.button(
                f"{icon} {voice['name']}\n{voice['desc']}",
                key=f"voice_{voice['id']}",
                type="primary" if is_selected else "secondary",
                use_container_width=True
            ):
                st.session_state["selected_edge_voice"] = voice["id"]
                st.rerun()

    # ì„ íƒëœ ìŒì„± í‘œì‹œ
    selected_voice_info = next((v for v in EDGE_VOICES.get(selected_lang, []) if v["id"] == selected_voice), None)
    if selected_voice_info:
        st.success(f"ì„ íƒëœ ìŒì„±: **{selected_voice_info['name']}** ({selected_voice_info['gender']}) - {selected_voice_info['desc']}")

    st.markdown("---")

    # === TTS ì„¤ì • ===
    st.markdown("#### âš™ï¸ ìŒì„± ì„¤ì •")

    col1, col2, col3 = st.columns(3)

    with col1:
        tts_rate = st.slider(
            "ğŸš€ ì†ë„",
            min_value=-50,
            max_value=100,
            value=st.session_state.get("edge_tts_rate", 0),
            step=5,
            format="%d%%",
            help="-50% (ëŠë¦¼) ~ +100% (ë¹ ë¦„)",
            key="edge_tts_rate_slider"
        )
        st.session_state["edge_tts_rate"] = tts_rate

    with col2:
        tts_pitch = st.slider(
            "ğŸµ í”¼ì¹˜",
            min_value=-50,
            max_value=50,
            value=st.session_state.get("edge_tts_pitch", 0),
            step=5,
            format="%dHz",
            help="-50Hz (ë‚®ìŒ) ~ +50Hz (ë†’ìŒ)",
            key="edge_tts_pitch_slider"
        )
        st.session_state["edge_tts_pitch"] = tts_pitch

    with col3:
        tts_volume = st.slider(
            "ğŸ”Š ë³¼ë¥¨",
            min_value=-50,
            max_value=50,
            value=st.session_state.get("edge_tts_volume", 0),
            step=5,
            format="%d%%",
            help="-50% (ì‘ìŒ) ~ +50% (í¼)",
            key="edge_tts_volume_slider"
        )
        st.session_state["edge_tts_volume"] = tts_volume

    # ì¶”ê°€ ì˜µì…˜
    col1, col2 = st.columns(2)

    with col1:
        add_breaks = st.checkbox(
            "ë¬¸ë‹¨/ë¬¸ì¥ ì‚¬ì´ì— ìë™ íœ´ì‹ ì‚½ì…",
            value=True,
            key="edge_add_breaks"
        )

    with col2:
        generate_subs = st.checkbox(
            "ìë§‰ íŒŒì¼ ìƒì„± (SRT)",
            value=True,
            key="edge_gen_subs"
        )

    st.markdown("---")

    # === ìŠ¤í¬ë¦½íŠ¸ ì…ë ¥ ===
    st.markdown("#### ğŸ“ ìŠ¤í¬ë¦½íŠ¸")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸ ì†ŒìŠ¤ ë™ì  ìƒì„±
    script_sources = ["ì§ì ‘ ì…ë ¥"]
    script_data = {}

    # 1. ìŠ¤í¬ë¦½íŠ¸ ìƒì„± íƒ­ ê²°ê³¼ (generated_script)
    if st.session_state.get("generated_script"):
        script_sources.append("ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ê²°ê³¼")
        script_data["ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ê²°ê³¼"] = st.session_state["generated_script"]

    # 2. ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (scene_analysis_script) - ê°œë³„ ì”¬ ë°ì´í„° í™•ì¸
    scenes_data = st.session_state.get("scenes", [])
    has_scene_data = len(scenes_data) > 0

    if st.session_state.get("scene_analysis_script") or has_scene_data:
        script_sources.append("ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸")
        script_data["ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸"] = st.session_state.get("scene_analysis_script", "")

    script_source = st.radio(
        "ìŠ¤í¬ë¦½íŠ¸ ì†ŒìŠ¤",
        options=script_sources,
        horizontal=True,
        key="edge_script_source"
    )

    # ì”¬ë³„ ìƒì„± ëª¨ë“œ ë³€ìˆ˜ ì´ˆê¸°í™”
    edge_generation_mode = "single"
    edge_selected_scenes = []
    script_text = ""

    if script_source == "ì§ì ‘ ì…ë ¥":
        script_text = st.text_area(
            "í…ìŠ¤íŠ¸ ì…ë ¥",
            height=200,
            placeholder="TTSë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
            key="edge_script_input"
        )
    elif script_source == "ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸" and has_scene_data:
        # ì”¬ë³„ ìƒì„± ëª¨ë“œ UI
        st.info(f"ğŸ“Š ì´ **{len(scenes_data)}ê°œ** ì”¬ì´ ë¶„ì„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ìƒì„± ëª¨ë“œ ì„ íƒ
        edge_generation_mode = st.radio(
            "ğŸ¯ ìƒì„± ëª¨ë“œ",
            options=["ì”¬ë³„ ê°œë³„ ìƒì„±", "ì „ì²´ í•©ì³ì„œ ìƒì„±"],
            horizontal=True,
            key="edge_generation_mode",
            help="ì”¬ë³„ ê°œë³„ ìƒì„±: ê° ì”¬ë§ˆë‹¤ ë³„ë„ ìŒì„± íŒŒì¼ ìƒì„±\nì „ì²´ í•©ì³ì„œ ìƒì„±: ëª¨ë“  ì”¬ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ìƒì„±"
        )

        st.markdown("---")

        if edge_generation_mode == "ì”¬ë³„ ê°œë³„ ìƒì„±":
            st.markdown("**ğŸ“‹ ìƒì„±í•  ì”¬ ì„ íƒ**")

            # ì „ì²´ ì„ íƒ/í•´ì œ
            col_sel1, col_sel2 = st.columns([1, 3])
            with col_sel1:
                select_all = st.checkbox("ì „ì²´ ì„ íƒ", value=True, key="edge_select_all_scenes")

            # ì”¬ ëª©ë¡ í‘œì‹œ
            edge_selected_scenes = []
            for idx, scene in enumerate(scenes_data):
                scene_id = scene.get('scene_id', idx + 1)
                scene_text = scene.get('script_text', '')
                char_count = len(scene_text)
                duration_est = scene.get('duration_estimate', char_count // 10)

                # ì²´í¬ë°•ìŠ¤ì™€ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ê°™ì€ í–‰ì—
                col_check, col_info = st.columns([1, 4])

                with col_check:
                    is_selected = st.checkbox(
                        f"ì”¬ {scene_id}",
                        value=select_all,
                        key=f"edge_scene_select_{scene_id}"
                    )

                with col_info:
                    with st.expander(f"{scene_text[:40]}... ({char_count}ì, ~{duration_est}ì´ˆ)", expanded=False):
                        st.text_area(
                            "ë‚´ìš©",
                            value=scene_text,
                            height=100,
                            disabled=True,
                            key=f"edge_scene_preview_{scene_id}"
                        )

                if is_selected:
                    edge_selected_scenes.append({
                        "scene_id": scene_id,
                        "text": scene_text,
                        "char_count": char_count,
                        "duration_estimate": duration_est
                    })

            # ì„ íƒ ìš”ì•½
            total_chars = sum(s["char_count"] for s in edge_selected_scenes)
            st.success(f"âœ… **{len(edge_selected_scenes)}ê°œ** ì”¬ ì„ íƒë¨ (ì´ {total_chars:,}ì)")

            # ì „ì²´ í…ìŠ¤íŠ¸ (ë¯¸ë¦¬ë³´ê¸°ìš©)
            script_text = "\n\n".join([s["text"] for s in edge_selected_scenes]) if edge_selected_scenes else ""

        else:
            # ì „ì²´ í•©ì³ì„œ ìƒì„± ëª¨ë“œ
            full_text = "\n\n".join([s.get('script_text', '') for s in scenes_data])
            script_text = full_text

            # ë©”íƒ€ ì •ë³´
            total_chars = sum(len(s.get('script_text', '')) for s in scenes_data)
            total_duration = sum(s.get('duration_estimate', 10) for s in scenes_data)

            cols = st.columns(3)
            cols[0].metric("ì´ ì”¬ ìˆ˜", f"{len(scenes_data)}ê°œ")
            cols[1].metric("ì´ ê¸€ì ìˆ˜", f"{total_chars:,}ì")
            cols[2].metric("ì˜ˆìƒ ê¸¸ì´", f"{total_duration // 60}ë¶„ {total_duration % 60}ì´ˆ")

            st.text_area(
                "ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ (ì½ê¸° ì „ìš©)",
                value=full_text,
                height=200,
                disabled=True,
                key="edge_full_script_preview"
            )

            # ì „ì²´ ì”¬ì„ ì„ íƒëœ ì”¬ìœ¼ë¡œ ì„¤ì •
            edge_selected_scenes = [{
                "scene_id": s.get('scene_id', idx + 1),
                "text": s.get('script_text', ''),
                "char_count": len(s.get('script_text', '')),
                "duration_estimate": s.get('duration_estimate', 10)
            } for idx, s in enumerate(scenes_data)]

    elif script_source in script_data:
        script_text = script_data[script_source]
        st.text_area(f"{script_source}", value=script_text, height=200, disabled=True, key="edge_script_preview")
    else:
        script_text = ""
        st.warning("ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë˜ëŠ” ì”¬ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.")

    # ë¬¸ì ìˆ˜
    if script_text:
        st.caption(f"ğŸ“Š {len(script_text)}ì | ì˜ˆìƒ ì‹œê°„: ì•½ {max(1, len(script_text) // 150)}ë¶„")

    st.markdown("---")

    # === ìƒì„± ë²„íŠ¼ ===
    # ì”¬ë³„ ê°œë³„ ìƒì„± ëª¨ë“œì¼ ë•Œ
    if script_source == "ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸" and has_scene_data and edge_generation_mode == "ì”¬ë³„ ê°œë³„ ìƒì„±":
        if st.button(
            f"ğŸµ Edge TTS ì”¬ë³„ ìƒì„± ({len(edge_selected_scenes)}ê°œ)",
            type="primary",
            use_container_width=True,
            disabled=len(edge_selected_scenes) == 0,
            key="generate_edge_tts_by_scenes"
        ):
            generate_edge_tts_by_scenes(
                scenes=edge_selected_scenes,
                voice_id=selected_voice,
                rate=tts_rate,
                pitch=tts_pitch,
                volume=tts_volume,
                add_breaks=add_breaks,
                generate_subs=generate_subs
            )
    else:
        # ì¼ë°˜ ìƒì„± ëª¨ë“œ
        if st.button(
            "ğŸµ Edge TTS ìƒì„±",
            type="primary",
            use_container_width=True,
            disabled=not script_text,
            key="generate_edge_tts"
        ):
            generate_edge_tts(
                text=script_text,
                voice_id=selected_voice,
                rate=tts_rate,
                pitch=tts_pitch,
                volume=tts_volume,
                add_breaks=add_breaks,
                generate_subs=generate_subs
            )


def generate_edge_tts(text, voice_id, rate, pitch, volume, add_breaks, generate_subs):
    """Edge TTS ìƒì„±"""
    import asyncio

    progress = st.progress(0)
    status = st.empty()

    # ìŒì„± ID ìœ íš¨ì„± ê²€ì‚¬
    all_valid_voices = []
    for lang_voices in EDGE_VOICES.values():
        all_valid_voices.extend([v["id"] for v in lang_voices])

    if voice_id not in all_valid_voices:
        status.error(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ìŒì„± ID: {voice_id}")
        st.warning("ì§€ì›ë˜ëŠ” ìŒì„± ëª©ë¡ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return

    status.text("Edge TTS ìƒì„± ì¤‘...")
    progress.progress(30)

    try:
        import edge_tts

        # ì„¤ì • ë¬¸ìì—´
        rate_str = f"{'+' if rate >= 0 else ''}{rate}%"
        pitch_str = f"{'+' if pitch >= 0 else ''}{pitch}Hz"
        volume_str = f"{'+' if volume >= 0 else ''}{volume}%"

        # ì¶œë ¥ ê²½ë¡œ
        output_dir = st.session_state.get("tts_output_dir", "data/tts")
        os.makedirs(output_dir, exist_ok=True)

        timestamp = int(time.time() * 1000)
        audio_path = os.path.join(output_dir, f"edge_tts_{timestamp}.mp3")
        srt_path = os.path.join(output_dir, f"edge_tts_{timestamp}.srt")

        async def generate():
            communicate = edge_tts.Communicate(
                text=text,
                voice=voice_id,
                rate=rate_str,
                volume=volume_str,
                pitch=pitch_str
            )

            if generate_subs:
                submaker = edge_tts.SubMaker()

                with open(audio_path, "wb") as f:
                    async for chunk in communicate.stream():
                        if chunk["type"] == "audio":
                            f.write(chunk["data"])
                        elif chunk["type"] == "WordBoundary":
                            submaker.create_sub(
                                (chunk["offset"], chunk["duration"]),
                                chunk["text"]
                            )

                # ìë§‰ ì €ì¥
                srt_content = ""
                if hasattr(submaker, 'generate_subs'):
                    try:
                        srt_content = submaker.generate_subs()
                    except:
                        pass
                if not srt_content and hasattr(submaker, 'get_srt'):
                    try:
                        srt_content = submaker.get_srt()
                    except:
                        pass
                if not srt_content:
                    try:
                        srt_content = str(submaker)
                    except:
                        pass

                if srt_content and srt_content.strip():
                    with open(srt_path, "w", encoding="utf-8") as f:
                        f.write(srt_content)
                    return audio_path, srt_path

                return audio_path, None
            else:
                await communicate.save(audio_path)
                return audio_path, None

        # ë¹„ë™ê¸° ì‹¤í–‰
        audio_path, subtitle_path = asyncio.run(generate())

        progress.progress(100)
        status.success("ìƒì„± ì™„ë£Œ!")

        # ê²°ê³¼ í‘œì‹œ
        st.audio(audio_path)

        col1, col2 = st.columns(2)

        with col1:
            with open(audio_path, "rb") as f:
                st.download_button(
                    "ğŸ’¾ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ (MP3)",
                    data=f,
                    file_name=f"edge_tts_{timestamp}.mp3",
                    mime="audio/mpeg",
                    use_container_width=True
                )

        with col2:
            if subtitle_path and os.path.exists(subtitle_path):
                with open(subtitle_path, "rb") as f:
                    st.download_button(
                        "ğŸ“„ ìë§‰ ë‹¤ìš´ë¡œë“œ (SRT)",
                        data=f,
                        file_name=f"edge_tts_{timestamp}.srt",
                        mime="text/plain",
                        use_container_width=True
                    )

        # ì„¸ì…˜ì— ì €ì¥
        st.session_state["last_tts_audio"] = audio_path
        st.session_state["last_tts_subtitle"] = subtitle_path

    except ImportError:
        status.error("edge-tts ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.code("pip install edge-tts")
    except Exception as e:
        error_msg = str(e)
        if "No audio was received" in error_msg:
            status.error("âš ï¸ ì˜¤ë””ì˜¤ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.warning(f"""
            **ê°€ëŠ¥í•œ ì›ì¸:**
            - ì„ íƒí•œ ìŒì„± ID({voice_id})ê°€ Microsoft Edge TTSì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
            - ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤

            **í•´ê²° ë°©ë²•:**
            - ë‹¤ë¥¸ ìŒì„±ì„ ì„ íƒí•´ë³´ì„¸ìš”
            - í•œêµ­ì–´ëŠ” ì„ í¬, ì¸ì¤€, í˜„ìˆ˜ 3ê°œ ìŒì„±ë§Œ ì§€ì›ë©ë‹ˆë‹¤
            """)
        else:
            status.error(f"ì˜¤ë¥˜: {e}")
        import traceback
        with st.expander("ìƒì„¸ ì˜¤ë¥˜"):
            st.code(traceback.format_exc())


def generate_edge_tts_by_scenes(scenes, voice_id, rate, pitch, volume, add_breaks, generate_subs):
    """ì”¬ë³„ Edge TTS ê°œë³„ ìƒì„±"""
    import asyncio

    # ìŒì„± ID ìœ íš¨ì„± ê²€ì‚¬
    all_valid_voices = []
    for lang_voices in EDGE_VOICES.values():
        all_valid_voices.extend([v["id"] for v in lang_voices])

    if voice_id not in all_valid_voices:
        st.error(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ìŒì„± ID: {voice_id}")
        st.warning("ì§€ì›ë˜ëŠ” ìŒì„± ëª©ë¡ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return

    # ì§„í–‰ ìƒí™© UI
    progress_bar = st.progress(0)
    status_text = st.empty()
    results_container = st.container()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = st.session_state.get("tts_output_dir", "data/tts")
    timestamp = int(time.time())
    scene_output_dir = os.path.join(output_dir, f"scenes_{timestamp}")
    os.makedirs(scene_output_dir, exist_ok=True)

    # ì„¤ì • ë¬¸ìì—´
    rate_str = f"{'+' if rate >= 0 else ''}{rate}%"
    pitch_str = f"{'+' if pitch >= 0 else ''}{pitch}Hz"
    volume_str = f"{'+' if volume >= 0 else ''}{volume}%"

    generated_files = []
    total_scenes = len(scenes)

    try:
        import edge_tts

        async def generate_single_scene(scene_data, output_path):
            """ë‹¨ì¼ ì”¬ TTS ìƒì„±"""
            text = scene_data.get("text", "")
            if not text.strip():
                return None

            communicate = edge_tts.Communicate(
                text=text,
                voice=voice_id,
                rate=rate_str,
                volume=volume_str,
                pitch=pitch_str
            )

            await communicate.save(output_path)
            return output_path

        # ì”¬ë³„ ìƒì„± ë£¨í”„
        for idx, scene in enumerate(scenes):
            scene_id = scene.get("scene_id", idx + 1)
            scene_text = scene.get("text", "")

            if not scene_text.strip():
                continue

            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            progress_bar.progress((idx + 1) / total_scenes)
            status_text.text(f"ì”¬ {scene_id} ìƒì„± ì¤‘... ({idx + 1}/{total_scenes})")

            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            audio_path = os.path.join(scene_output_dir, f"scene_{scene_id:02d}.mp3")

            try:
                # ë¹„ë™ê¸° ì‹¤í–‰
                asyncio.run(generate_single_scene(scene, audio_path))

                generated_files.append({
                    "scene_id": scene_id,
                    "path": audio_path,
                    "text_preview": scene_text[:50] + "..." if len(scene_text) > 50 else scene_text,
                    "char_count": len(scene_text),
                    "status": "success"
                })

            except Exception as e:
                generated_files.append({
                    "scene_id": scene_id,
                    "path": None,
                    "error": str(e),
                    "status": "failed"
                })

        progress_bar.progress(1.0)
        status_text.empty()

        # ê²°ê³¼ í‘œì‹œ
        success_count = len([f for f in generated_files if f["status"] == "success"])
        failed_count = len([f for f in generated_files if f["status"] == "failed"])

        with results_container:
            if success_count > 0:
                st.success(f"âœ… **{success_count}/{total_scenes}ê°œ** ì”¬ ìƒì„± ì™„ë£Œ!")

                # ì”¬ë³„ ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ë° ë‹¤ìš´ë¡œë“œ
                st.markdown("### ğŸµ ìƒì„±ëœ ìŒì„± íŒŒì¼")

                for file_info in generated_files:
                    scene_id = file_info["scene_id"]

                    if file_info["status"] == "success":
                        with st.expander(f"ğŸ“¢ ì”¬ {scene_id} - {file_info['text_preview']} ({file_info['char_count']}ì)", expanded=True):
                            col1, col2 = st.columns([3, 1])

                            with col1:
                                st.audio(file_info["path"])

                            with col2:
                                with open(file_info["path"], "rb") as f:
                                    st.download_button(
                                        "â¬‡ï¸ ë‹¤ìš´ë¡œë“œ",
                                        data=f.read(),
                                        file_name=f"scene_{scene_id:02d}.mp3",
                                        mime="audio/mpeg",
                                        key=f"download_scene_{scene_id}_{timestamp}",
                                        use_container_width=True
                                    )
                    else:
                        st.error(f"âŒ ì”¬ {scene_id} ìƒì„± ì‹¤íŒ¨: {file_info.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

                # ì „ì²´ ZIP ë‹¤ìš´ë¡œë“œ
                if success_count > 1:
                    st.markdown("---")
                    st.markdown("### ğŸ“¦ ì¼ê´„ ë‹¤ìš´ë¡œë“œ")

                    # ZIP íŒŒì¼ ìƒì„±
                    import zipfile
                    import io

                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for file_info in generated_files:
                            if file_info["status"] == "success" and file_info["path"]:
                                scene_id = file_info["scene_id"]
                                zip_file.write(file_info["path"], f"scene_{scene_id:02d}.mp3")

                    zip_buffer.seek(0)

                    st.download_button(
                        f"ğŸ“¦ ì „ì²´ ë‹¤ìš´ë¡œë“œ (ZIP, {success_count}ê°œ íŒŒì¼)",
                        data=zip_buffer.getvalue(),
                        file_name=f"tts_scenes_{timestamp}.zip",
                        mime="application/zip",
                        key=f"download_all_zip_{timestamp}",
                        use_container_width=True
                    )

                # ì„¸ì…˜ì— ì €ì¥
                st.session_state["last_tts_scenes"] = generated_files
                st.session_state["last_tts_output_dir"] = scene_output_dir

            if failed_count > 0:
                st.warning(f"âš ï¸ {failed_count}ê°œ ì”¬ ìƒì„± ì‹¤íŒ¨")

    except ImportError:
        status_text.error("edge-tts ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.code("pip install edge-tts")
    except Exception as e:
        status_text.error(f"ì˜¤ë¥˜: {e}")
        import traceback
        with st.expander("ìƒì„¸ ì˜¤ë¥˜"):
            st.code(traceback.format_exc())


# ============================================================
# Chatterbox íƒ­ - ìŒì„± í´ë¡  ê´€ë¦¬
# ============================================================

def get_voice_samples_dir():
    """í”„ë¡œì íŠ¸ë³„ ìŒì„± ìƒ˜í”Œ ë””ë ‰í† ë¦¬ ë°˜í™˜ (ì ˆëŒ€ ê²½ë¡œ)"""
    # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
    base_dir = Path(__file__).parent.parent.resolve()  # longform ë£¨íŠ¸ ë””ë ‰í† ë¦¬

    current_project = st.session_state.get("current_project")
    if current_project:
        samples_dir = base_dir / "data" / "projects" / current_project / "voice_samples"
    else:
        samples_dir = base_dir / "data" / "voice_samples" / "default"
    samples_dir.mkdir(parents=True, exist_ok=True)
    return samples_dir


def get_voice_samples(samples_dir: Path) -> list:
    """ìŒì„± ìƒ˜í”Œ ëª©ë¡ ì¡°íšŒ"""
    import json

    samples = []
    meta_path = samples_dir / "samples_meta.json"

    if meta_path.exists():
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)

        for s in meta.get("samples", []):
            filepath = samples_dir / s["filename"]
            if filepath.exists():
                samples.append({
                    "name": s["name"],
                    "path": str(filepath.resolve()),  # ì ˆëŒ€ ê²½ë¡œ í™•ì‹¤íˆ ì ìš©
                    "description": s.get("description", ""),
                    "created_at": s.get("created_at", "")
                })
    else:
        # ë©”íƒ€ ì—†ìœ¼ë©´ íŒŒì¼ ì§ì ‘ ìŠ¤ìº”
        for f in samples_dir.glob("*"):
            if f.suffix.lower() in ['.wav', '.mp3', '.m4a', '.ogg']:
                samples.append({
                    "name": f.stem,
                    "path": str(f.resolve()),  # ì ˆëŒ€ ê²½ë¡œ í™•ì‹¤íˆ ì ìš©
                    "description": "",
                    "created_at": ""
                })

    # voice_library/ko í´ë”ë„ í¬í•¨ (ì ˆëŒ€ ê²½ë¡œ)
    base_dir = Path(__file__).parent.parent.resolve()
    voice_lib = base_dir / "voice_library" / "ko"
    if voice_lib.exists():
        for f in voice_lib.glob("*"):
            if f.suffix.lower() in ['.wav', '.mp3', '.m4a', '.ogg']:
                samples.append({
                    "name": f"[ë¼ì´ë¸ŒëŸ¬ë¦¬] {f.stem}",
                    "path": str(f.resolve()),  # ì ˆëŒ€ ê²½ë¡œ í™•ì‹¤íˆ ì ìš©
                    "description": "ê¸°ë³¸ ìŒì„± ë¼ì´ë¸ŒëŸ¬ë¦¬",
                    "created_at": ""
                })

    return samples


def save_voice_sample(uploaded_file, name: str, description: str, samples_dir: Path):
    """ìŒì„± ìƒ˜í”Œ ì €ì¥"""
    import json

    # íŒŒì¼ ì €ì¥
    ext = uploaded_file.name.rsplit('.', 1)[-1]
    filename = f"{name.replace(' ', '_')}.{ext}"
    filepath = samples_dir / filename

    with open(filepath, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    meta_path = samples_dir / "samples_meta.json"

    if meta_path.exists():
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
    else:
        meta = {"samples": []}

    # ì¤‘ë³µ ì œê±°
    meta["samples"] = [s for s in meta["samples"] if s["filename"] != filename]

    meta["samples"].append({
        "name": name,
        "filename": filename,
        "description": description,
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
    })

    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    st.success(f"âœ… '{name}' ìƒ˜í”Œì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    time.sleep(0.5)
    st.rerun()


def delete_voice_sample(filepath: str):
    """ìŒì„± ìƒ˜í”Œ ì‚­ì œ"""
    import json

    filepath = Path(filepath)

    if filepath.exists():
        filepath.unlink()

    # ë©”íƒ€ë°ì´í„°ì—ì„œë„ ì œê±°
    samples_dir = filepath.parent
    meta_path = samples_dir / "samples_meta.json"

    if meta_path.exists():
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)

        filename = filepath.name
        meta["samples"] = [s for s in meta["samples"] if s["filename"] != filename]

        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

    st.success("ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
    time.sleep(0.5)
    st.rerun()


def render_voice_clone_manager():
    """ğŸ­ ìŒì„± í´ë¡  ê´€ë¦¬ ì„¹ì…˜"""
    st.markdown("### ğŸ­ ìŒì„± í´ë¡  ê´€ë¦¬")

    samples_dir = get_voice_samples_dir()

    # íƒ­: ì—…ë¡œë“œ / ë…¹ìŒ / ê´€ë¦¬
    clone_tabs = st.tabs(["ğŸ“¤ ì—…ë¡œë“œ", "ğŸ™ï¸ ë…¹ìŒ", "ğŸ“‹ í´ë¡  ëª©ë¡"])

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íƒ­ 1: ìŒì„± ìƒ˜í”Œ ì—…ë¡œë“œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with clone_tabs[0]:
        st.markdown("#### ğŸ“¤ ìƒˆ ìŒì„± ìƒ˜í”Œ ì—…ë¡œë“œ")
        st.info("ğŸ’¡ 3~10ì´ˆ ê¸¸ì´ì˜ ê¹¨ë—í•œ ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. (WAV/MP3 ê¶Œì¥)")

        uploaded_file = st.file_uploader(
            "ìŒì„± íŒŒì¼ ì„ íƒ",
            type=["wav", "mp3", "m4a", "ogg"],
            key="voice_sample_upload"
        )

        if uploaded_file:
            # ë¯¸ë¦¬ë“£ê¸°
            st.audio(uploaded_file)

            col1, col2 = st.columns(2)

            with col1:
                sample_name = st.text_input(
                    "ìƒ˜í”Œ ì´ë¦„",
                    value=uploaded_file.name.rsplit('.', 1)[0],
                    key="sample_name_input"
                )

            with col2:
                sample_desc = st.text_input(
                    "ì„¤ëª… (ì„ íƒ)",
                    placeholder="ì˜ˆ: ë°ì€ í†¤, ì°¨ë¶„í•œ ëª©ì†Œë¦¬",
                    key="sample_desc_input"
                )

            if st.button("ğŸ’¾ ìƒ˜í”Œ ì €ì¥", type="primary", use_container_width=True, key="save_sample"):
                if sample_name:
                    save_voice_sample(uploaded_file, sample_name, sample_desc, samples_dir)
                else:
                    st.warning("ìƒ˜í”Œ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íƒ­ 2: ìŒì„± ë…¹ìŒ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with clone_tabs[1]:
        st.markdown("#### ğŸ™ï¸ ìŒì„± ë…¹ìŒ")

        # audiorecorder ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„
        try:
            from audiorecorder import audiorecorder

            st.info("ğŸ’¡ ğŸ”´ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë…¹ìŒì„ ì‹œì‘/ì¤‘ì§€í•˜ì„¸ìš”.")

            audio = audiorecorder("ğŸ”´ ë…¹ìŒ ì‹œì‘", "â¹ï¸ ë…¹ìŒ ì¤‘ì§€", key="voice_recorder")

            if len(audio) > 0:
                st.audio(audio.export().read())

                col1, col2 = st.columns(2)

                with col1:
                    rec_name = st.text_input("ë…¹ìŒ ì´ë¦„", key="rec_name_input")

                with col2:
                    rec_desc = st.text_input("ì„¤ëª…", key="rec_desc_input")

                if st.button("ğŸ’¾ ë…¹ìŒ ì €ì¥", type="primary", key="save_recording"):
                    if rec_name:
                        # WAVë¡œ ì €ì¥
                        filepath = samples_dir / f"{rec_name}.wav"
                        audio.export(str(filepath), format="wav")

                        # ë©”íƒ€ë°ì´í„° ì €ì¥
                        import json
                        meta_path = samples_dir / "samples_meta.json"

                        if meta_path.exists():
                            with open(meta_path, "r", encoding="utf-8") as f:
                                meta = json.load(f)
                        else:
                            meta = {"samples": []}

                        meta["samples"].append({
                            "name": rec_name,
                            "filename": f"{rec_name}.wav",
                            "description": rec_desc,
                            "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
                        })

                        with open(meta_path, "w", encoding="utf-8") as f:
                            json.dump(meta, f, ensure_ascii=False, indent=2)

                        st.success("ë…¹ìŒì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        time.sleep(0.5)
                        st.rerun()
                    else:
                        st.warning("ë…¹ìŒ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.")

        except ImportError:
            st.warning("ë…¹ìŒ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
            st.code("pip install streamlit-audiorecorder")

            st.markdown("---")
            st.markdown("**ëŒ€ì•ˆ: ë…¹ìŒ íŒŒì¼ ì§ì ‘ ì—…ë¡œë“œ**")
            st.info("íœ´ëŒ€í°ì´ë‚˜ ë‹¤ë¥¸ ê¸°ê¸°ë¡œ ë…¹ìŒ í›„ 'ğŸ“¤ ì—…ë¡œë“œ' íƒ­ì—ì„œ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íƒ­ 3: í´ë¡  ëª©ë¡ ê´€ë¦¬
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with clone_tabs[2]:
        st.markdown("#### ğŸ“‹ ì €ì¥ëœ ìŒì„± í´ë¡ ")

        samples = get_voice_samples(samples_dir)

        if not samples:
            st.info("ì €ì¥ëœ ìŒì„± ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë…¹ìŒí•˜ì„¸ìš”.")
        else:
            st.caption(f"ì´ {len(samples)}ê°œì˜ ìŒì„± ìƒ˜í”Œ")

            for i, sample in enumerate(samples):
                with st.container():
                    col1, col2, col3, col4 = st.columns([4, 1, 1, 1])

                    with col1:
                        st.markdown(f"**{sample['name']}**")
                        if sample.get('description'):
                            st.caption(sample['description'])

                    with col2:
                        # ë¯¸ë¦¬ë“£ê¸°
                        if st.button("â–¶ï¸", key=f"play_sample_{i}", help="ë¯¸ë¦¬ë“£ê¸°"):
                            st.session_state[f"preview_sample_{i}"] = True

                    with col3:
                        # ê¸°ë³¸ ìŒì„±ìœ¼ë¡œ ì„¤ì •
                        is_default = sample['path'] == st.session_state.get("default_voice_sample")
                        if st.button(
                            "â­" if is_default else "â˜†",
                            key=f"default_sample_{i}",
                            help="ê¸°ë³¸ ìŒì„±ìœ¼ë¡œ ì„¤ì •"
                        ):
                            st.session_state["default_voice_sample"] = sample['path']
                            st.toast(f"'{sample['name']}'ì„ ê¸°ë³¸ ìŒì„±ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")

                    with col4:
                        # ì‚­ì œ (ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒŒì¼ì€ ì‚­ì œ ë¶ˆê°€)
                        if "[ë¼ì´ë¸ŒëŸ¬ë¦¬]" not in sample['name']:
                            if st.button("ğŸ—‘ï¸", key=f"delete_sample_{i}", help="ì‚­ì œ"):
                                delete_voice_sample(sample['path'])
                        else:
                            st.caption("ğŸ”’")

                    # ë¯¸ë¦¬ë“£ê¸° ì˜¤ë””ì˜¤
                    if st.session_state.get(f"preview_sample_{i}"):
                        st.audio(sample['path'])
                        st.session_state[f"preview_sample_{i}"] = False

                    st.markdown("---")


def render_reference_voice_selector():
    """ì°¸ì¡° ìŒì„± ì„ íƒ (ê°œì„ ëœ ë²„ì „ + ìŒì„± ë¶„ì„)"""
    st.markdown("#### ğŸ¤ ì°¸ì¡° ìŒì„± ì„ íƒ")

    samples_dir = get_voice_samples_dir()
    samples = get_voice_samples(samples_dir)

    if not samples:
        st.warning("ì €ì¥ëœ ìŒì„± ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ 'ìŒì„± í´ë¡  ê´€ë¦¬'ì—ì„œ ë¨¼ì € ìƒ˜í”Œì„ ì¶”ê°€í•˜ì„¸ìš”.")
        st.session_state["selected_reference_voice"] = None
        return None

    sample_options = ["ì—†ìŒ (ê¸°ë³¸ ìŒì„±)"] + [s['name'] for s in samples]
    sample_paths = {s['name']: s['path'] for s in samples}
    path_to_name = {s['path']: s['name'] for s in samples}

    # ì´ˆê¸° ì¸ë±ìŠ¤ ê²°ì •: ì´ì „ ì„ íƒ > ê¸°ë³¸ ìŒì„± > 0
    initial_index = 0

    # 1. ì´ì „ì— ì„ íƒëœ ìŒì„±ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„ 
    stored_selection = st.session_state.get("ref_voice_select")
    if stored_selection and stored_selection in sample_options:
        initial_index = sample_options.index(stored_selection)
    else:
        # 2. ì„¸ì…˜ì— ì €ì¥ëœ ì°¸ì¡° ìŒì„± ê²½ë¡œ í™•ì¸
        stored_ref_path = st.session_state.get("selected_reference_voice")
        if stored_ref_path and stored_ref_path in path_to_name:
            stored_name = path_to_name[stored_ref_path]
            if stored_name in sample_options:
                initial_index = sample_options.index(stored_name)
        else:
            # 3. ê¸°ë³¸ ìŒì„± ì„¤ì • í™•ì¸
            default_voice = st.session_state.get("default_voice_sample")
            if default_voice and default_voice in path_to_name:
                default_name = path_to_name[default_voice]
                if default_name in sample_options:
                    initial_index = sample_options.index(default_name)

    selected_name = st.selectbox(
        "ì°¸ì¡° ìŒì„±",
        options=sample_options,
        index=initial_index,
        key="ref_voice_select"
    )

    # ë””ë²„ê·¸ ë¡œê¹…
    print(f"[VoiceSelector] selected_name: {selected_name}")

    if selected_name and selected_name != "ì—†ìŒ (ê¸°ë³¸ ìŒì„±)":
        selected_path = sample_paths.get(selected_name)
        print(f"[VoiceSelector] selected_path from dict: {selected_path}")

        if selected_path:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if os.path.exists(selected_path):
                # ì„ íƒëœ ìŒì„± ì •ë³´ í‘œì‹œ
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.audio(selected_path)
                with col2:
                    st.success(f"âœ“ {selected_name}")

                st.session_state["selected_reference_voice"] = selected_path

                # â­ í…ìŠ¤íŠ¸ ì…ë ¥ UI (ì •í™•í•œ ë°œí™”ì†ë„ ì¸¡ì •ìš©)
                current_transcript = get_voice_transcript(selected_path) or ""
                has_transcript = bool(current_transcript)

                with st.expander(
                    f"ğŸ“ í…ìŠ¤íŠ¸ {'í¸ì§‘' if has_transcript else 'ì…ë ¥'} (ì •í™•í•œ ë°œí™”ì†ë„ ì¸¡ì •)",
                    expanded=not has_transcript  # í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ í¼ì³ì„œ ì…ë ¥ ìœ ë„
                ):
                    if has_transcript:
                        st.success(f"âœ… í…ìŠ¤íŠ¸ ë“±ë¡ë¨ ({len(current_transcript)}ì)")
                    else:
                        st.warning("âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ - ë°œí™”ì†ë„ ì¶”ì • ëª¨ë“œ (ì •í™•ë„ Â±20%)")
                        st.caption("ì°¸ì¡° ìŒì„±ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ì •í™•í•œ ë°œí™”ì†ë„ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    new_transcript = st.text_area(
                        "ì°¸ì¡° ìŒì„± í…ìŠ¤íŠ¸",
                        value=current_transcript,
                        height=80,
                        placeholder="ì˜ˆ: ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ì€ íšŒê³„ì‚¬ ì‹œí—˜ ì¤€ë¹„ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ ë³´ë ¤ê³  í•©ë‹ˆë‹¤.",
                        key="transcript_input",
                        label_visibility="collapsed"
                    )

                    col_save, col_clear = st.columns(2)
                    with col_save:
                        if st.button("ğŸ’¾ í…ìŠ¤íŠ¸ ì €ì¥", key="save_transcript", use_container_width=True):
                            if new_transcript.strip():
                                set_voice_transcript(selected_path, new_transcript.strip())
                                # ì¬ë¶„ì„ ê°•ì œ
                                st.session_state["_prev_analyzed_voice_path"] = None
                                st.success("âœ… í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ! ì¬ë¶„ì„ ì¤‘...")
                                st.rerun()
                            else:
                                st.error("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
                    with col_clear:
                        if has_transcript:
                            if st.button("ğŸ—‘ï¸ í…ìŠ¤íŠ¸ ì‚­ì œ", key="delete_transcript", use_container_width=True):
                                set_voice_transcript(selected_path, "")
                                st.session_state["_prev_analyzed_voice_path"] = None
                                st.rerun()

                # â­ ìŒì„± ë¶„ì„ ë° íŒŒë¼ë¯¸í„° ì¶”ì²œ
                prev_analyzed_path = st.session_state.get("_prev_analyzed_voice_path")
                if selected_path != prev_analyzed_path:
                    # ìƒˆ ìŒì„± ì„ íƒë¨ â†’ ë¶„ì„ ì‹¤í–‰
                    _analyze_and_update_params(selected_path, selected_name)
                    st.session_state["_prev_analyzed_voice_path"] = selected_path

                # â­ ë¶„ì„ ê²°ê³¼ í‘œì‹œ (ì •í™•/ì¶”ì • êµ¬ë¶„)
                if "voice_analysis" in st.session_state:
                    analysis = st.session_state["voice_analysis"]
                    tempo = analysis.get("tempo", "normal")
                    speech_rate = analysis.get("speech_rate", 8.5)
                    accurate = analysis.get("speech_rate_accurate", False)

                    tempo_emoji = {"slow": "ğŸ¢", "normal": "ğŸš¶", "fast": "ğŸƒ"}.get(tempo, "ğŸš¶")
                    tempo_kr = {"slow": "ëŠë¦¼", "normal": "ë³´í†µ", "fast": "ë¹ ë¦„"}.get(tempo, "ë³´í†µ")

                    if accurate:
                        st.success(f"â­ **ì •í™•í•œ ì¸¡ì •**: {speech_rate:.2f} ê¸€ì/ì´ˆ ({tempo_emoji} {tempo_kr}) â†’ íŒŒë¼ë¯¸í„° ìë™ ì¡°ì •ë¨")
                    else:
                        st.info(f"ğŸ“Š **ì¶”ì • ì¸¡ì •**: {speech_rate:.1f} ê¸€ì/ì´ˆ ({tempo_emoji} {tempo_kr}) â†’ íŒŒë¼ë¯¸í„° ìë™ ì¡°ì •ë¨")

                print(f"[VoiceSelector] âœ… ë°˜í™˜: {selected_path}")
                return selected_path
            else:
                st.warning(f"âš ï¸ ìŒì„± íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {selected_path}")
                print(f"[VoiceSelector] âŒ íŒŒì¼ ì—†ìŒ: {selected_path}")
        else:
            st.warning(f"âš ï¸ ì„ íƒëœ ìŒì„± '{selected_name}'ì˜ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"[VoiceSelector] âŒ ê²½ë¡œ ë§¤í•‘ ì‹¤íŒ¨: {selected_name}")

    # ê¸°ë³¸ ìŒì„± ì„ íƒë¨ â†’ ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ë¦¬ì…‹
    st.session_state["selected_reference_voice"] = None
    st.session_state["voice_analysis"] = None
    st.session_state["recommended_params"] = None
    st.session_state["_prev_analyzed_voice_path"] = None
    st.info("ê¸°ë³¸ Chatterbox ìŒì„±ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.")
    print("[VoiceSelector] ê¸°ë³¸ ìŒì„± ë°˜í™˜ (None)")
    return None


def _analyze_and_update_params(voice_path: str, voice_name: str):
    """
    ì°¸ì¡° ìŒì„± ë¶„ì„ í›„ ì„¸ì…˜ ìƒíƒœì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

    â­ í•µì‹¬: ìŒì„± íŠ¹ì„±ì— ë§ëŠ” íŒŒë¼ë¯¸í„° ìë™ ì„¤ì •
    """
    print(f"\n[VoiceAnalysis] ìŒì„± ë¶„ì„ ì‹œì‘: {voice_name}")

    try:
        result = analyze_voice_and_get_params(voice_path)

        analysis = result.get("analysis", {})
        params = result.get("recommended_params", {})

        # ì„¸ì…˜ì— ì €ì¥
        st.session_state["voice_analysis"] = analysis
        st.session_state["recommended_params"] = params

        # â­ íŒŒë¼ë¯¸í„° ìë™ ì—…ë°ì´íŠ¸ (ìŠ¬ë¼ì´ë” ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë¨)
        if "speed" in params:
            st.session_state["chatter_speed"] = params["speed"]
        if "cfg_weight" in params:
            st.session_state["chatter_cfg"] = params["cfg_weight"]
        if "exaggeration" in params:
            st.session_state["chatter_exag"] = params["exaggeration"]
        if "temperature" in params:
            st.session_state["chatter_temp"] = params["temperature"]
        if "target_speed" in params:
            st.session_state["target_speech_rate"] = params["target_speed"]

        tempo = analysis.get("tempo", "normal")
        speech_rate = analysis.get("speech_rate", 8.5)
        tempo_kr = {"slow": "ëŠë¦¼", "normal": "ë³´í†µ", "fast": "ë¹ ë¦„"}.get(tempo, "ë³´í†µ")

        print(f"[VoiceAnalysis] ì™„ë£Œ: {tempo_kr} ({speech_rate:.1f} ê¸€ì/ì´ˆ)")
        print(f"[VoiceAnalysis] ì¶”ì²œ íŒŒë¼ë¯¸í„°: {params}")

    except Exception as e:
        print(f"[VoiceAnalysis] âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ê°’ ìœ ì§€


# ============================================================
# Chatterbox íƒ­
# ============================================================

def render_chatterbox_tab():
    """Chatterbox íƒ­ ë Œë”ë§"""
    st.markdown("### ğŸ¤ Chatterbox TTS")
    st.info("ChatterboxëŠ” ê³ í’ˆì§ˆ ìŒì„± í•©ì„± ì„œë²„ì…ë‹ˆë‹¤. ë¡œì»¬ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    # ì„œë²„ ìƒíƒœ í™•ì¸ (ìºì‹± ì ìš©)
    server_status = check_chatterbox_server()

    # ì„œë²„ ìƒíƒœ í‘œì‹œ ì˜ì—­
    status_container = st.container()

    if not server_status:
        with status_container:
            st.error("âŒ Chatterbox ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸš€ ì„œë²„ ì‹œì‘ (ìƒˆ ì°½)", type="primary", use_container_width=True):
                try:
                    import subprocess
                    subprocess.Popen(
                        'start cmd /k "cd /d C:\\Users\\KIMJAEHEON\\chatter && call venv\\Scripts\\activate.bat && python run.py"',
                        shell=True
                    )
                    st.success("ì„œë²„ ì‹œì‘ ëª…ë ¹ ì „ì†¡!")
                    st.info("ìƒˆ ì½˜ì†” ì°½ì—ì„œ ì„œë²„ê°€ ì‹œì‘ë©ë‹ˆë‹¤. ì ì‹œ í›„ 'ì—°ê²° í™•ì¸' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜: {e}")

        with col2:
            if st.button("ğŸ”„ ì—°ê²° í™•ì¸", use_container_width=True, key="check_connection_btn"):
                # ìºì‹œ ë¬´íš¨í™” í›„ ê°•ì œ ìƒˆë¡œê³ ì¹¨
                invalidate_chatterbox_cache()
                with st.spinner("ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘..."):
                    new_status = check_chatterbox_server(force_refresh=True)
                if new_status:
                    st.success("ì„œë²„ ì—°ê²°ë¨!")
                    st.rerun()
                else:
                    st.error("ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        with col3:
            st.caption("ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ:")
            st.code("cd C:\\Users\\KIMJAEHEON\\chatter\npython run.py", language="bash")

        return

    # ì„œë²„ ì—°ê²°ë¨ - ìƒíƒœ í‘œì‹œ
    with status_container:
        col_status1, col_status2 = st.columns([3, 1])
        with col_status1:
            st.success("âœ… Chatterbox ì„œë²„ ì—°ê²°ë¨")
        with col_status2:
            if st.button("ğŸ”„", key="refresh_status_btn", help="ìƒíƒœ ìƒˆë¡œê³ ì¹¨"):
                invalidate_chatterbox_cache()
                st.rerun()

    # ì„œë²„ ìƒíƒœ ì •ë³´ (ìºì‹± ì ìš©)
    status = get_chatterbox_status()
    if status:
        model_loaded = status.get("model_loaded", False)
        if model_loaded:
            st.success("ğŸŸ¢ ëª¨ë¸ ë¡œë“œë¨ - TTS ìƒì„± ì¤€ë¹„ ì™„ë£Œ")
        else:
            st.warning("ğŸŸ¡ ëª¨ë¸ ë¯¸ë¡œë“œ - TTS ì‚¬ìš© ì „ ëª¨ë¸ ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤")
            if st.button("ğŸ“¥ ëª¨ë¸ ë¡œë“œ", key="load_chatterbox_model", type="primary"):
                progress_bar = st.progress(0, text="ëª¨ë¸ ë¡œë”© ì¤‘...")
                status_text = st.empty()

                status_text.text("ğŸ”„ Chatterbox ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒë§Œ í•„ìš”, ì•½ 30ì´ˆ~1ë¶„ ì†Œìš”)")
                progress_bar.progress(10)

                try:
                    progress_bar.progress(30, text="ì„œë²„ì— ë¡œë“œ ìš”ì²­ ì¤‘...")
                    r = requests.post(f"{CHATTERBOX_URL}/load", timeout=180)
                    progress_bar.progress(80, text="ë¡œë“œ ì™„ë£Œ í™•ì¸ ì¤‘...")

                    if r.status_code == 200:
                        progress_bar.progress(100, text="ì™„ë£Œ!")
                        status_text.success("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                        # ìºì‹œ ë¬´íš¨í™”
                        invalidate_chatterbox_cache()
                        time.sleep(1)
                        st.rerun()
                    else:
                        status_text.error(f"ë¡œë“œ ì‹¤íŒ¨: HTTP {r.status_code}")
                except requests.exceptions.Timeout:
                    status_text.error("â±ï¸ ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (180ì´ˆ). ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                except Exception as e:
                    status_text.error(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")

    st.markdown("---")

    # =========================================================
    # ğŸ­ ìŒì„± í´ë¡  ê´€ë¦¬ (í•µì‹¬ ê¸°ëŠ¥!)
    # =========================================================
    with st.expander("ğŸ­ ìŒì„± í´ë¡  ê´€ë¦¬", expanded=True):
        render_voice_clone_manager()

    st.markdown("---")

    # === ìŒì„± íŒŒë¼ë¯¸í„° ===
    st.markdown("#### ğŸšï¸ ìŒì„± íŒŒë¼ë¯¸í„°")

    # ë¹ ë¥¸ í”„ë¦¬ì…‹
    preset_cols = st.columns(5)

    with preset_cols[0]:
        if st.button("ğŸ¢ ì°¨ë¶„", use_container_width=True, key="preset_calm"):
            st.session_state["chatter_exag"] = 0.3
            st.session_state["chatter_speed"] = 0.9

    with preset_cols[1]:
        if st.button("âš¡ ë¹ ë¦„", use_container_width=True, key="preset_fast"):
            st.session_state["chatter_speed"] = 1.3

    with preset_cols[2]:
        if st.button("ğŸ˜Š ê°ì •", use_container_width=True, key="preset_emotion"):
            st.session_state["chatter_exag"] = 0.7

    with preset_cols[3]:
        if st.button("ğŸ¯ ì •í™•", use_container_width=True, key="preset_precise"):
            st.session_state["chatter_cfg"] = 0.7

    with preset_cols[4]:
        if st.button("ğŸ”„ ì´ˆê¸°í™”", use_container_width=True, key="preset_reset"):
            st.session_state["chatter_exag"] = 0.5
            st.session_state["chatter_cfg"] = 0.5
            st.session_state["chatter_speed"] = 1.0
            st.session_state["chatter_temp"] = 0.8

    col1, col2 = st.columns(2)

    with col1:
        cfg_weight = st.slider(
            "ğŸ¯ CFG Weight (í’ˆì§ˆ/ì†ë„)",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.get("chatter_cfg", 0.5),
            step=0.05,
            key="chatter_cfg_slider"
        )
        st.session_state["chatter_cfg"] = cfg_weight

        speed = st.slider(
            "â±ï¸ ë§í•˜ê¸° ì†ë„",
            min_value=0.5,
            max_value=2.0,
            value=st.session_state.get("chatter_speed", 1.0),
            step=0.05,
            key="chatter_speed_slider"
        )
        st.session_state["chatter_speed"] = speed

    with col2:
        exaggeration = st.slider(
            "ğŸ˜Š ê°ì • ê°•ë„ (Exaggeration)",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.get("chatter_exag", 0.5),
            step=0.05,
            key="chatter_exag_slider"
        )
        st.session_state["chatter_exag"] = exaggeration

        temperature = st.slider(
            "ğŸŒ¡ï¸ Temperature (ë‹¤ì–‘ì„±)",
            min_value=0.3,
            max_value=1.5,
            value=st.session_state.get("chatter_temp", 0.85),  # â­ 0.8â†’0.85 ìì—°ìŠ¤ëŸ¬ì›€ ìµœì í™”
            step=0.05,
            key="chatter_temp_slider",
            help="ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ í†¤. 0.85 ê¶Œì¥ (ê¸°ì¡´ 0.8ì€ ë‹¨ì¡°ë¡œì›€)"
        )
        st.session_state["chatter_temp"] = temperature

    # ì‹œë“œ ì„¤ì •
    col1, col2 = st.columns([1, 1])
    with col1:
        use_random_seed = st.checkbox("ğŸ² ëœë¤ ì‹œë“œ", value=True, key="chatter_random_seed")
    with col2:
        if not use_random_seed:
            seed = st.number_input("Seed", min_value=0, value=42, key="chatter_seed_input")
        else:
            seed = None

    st.markdown("---")

    # === ì°¸ì¡° ìŒì„± ì„ íƒ (ê°œì„ ëœ ë²„ì „) ===
    voice_path = render_reference_voice_selector()

    st.markdown("---")

    # === í…ìŠ¤íŠ¸ ì…ë ¥ ===
    st.markdown("#### ğŸ“ í…ìŠ¤íŠ¸ ì…ë ¥")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸ ì†ŒìŠ¤ ë™ì  ìƒì„±
    chatter_script_sources = ["ì§ì ‘ ì…ë ¥"]
    chatter_script_data = {}

    # 1. ìŠ¤í¬ë¦½íŠ¸ ìƒì„± íƒ­ ê²°ê³¼ (generated_script)
    if st.session_state.get("generated_script"):
        chatter_script_sources.append("ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ê²°ê³¼")
        chatter_script_data["ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ê²°ê³¼"] = st.session_state["generated_script"]

    # 2. ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (scene_analysis_script) - ê°œë³„ ì”¬ ë°ì´í„° í™•ì¸
    chatter_scenes_data = st.session_state.get("scenes", [])
    chatter_has_scene_data = len(chatter_scenes_data) > 0

    if st.session_state.get("scene_analysis_script") or chatter_has_scene_data:
        chatter_script_sources.append("ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸")
        chatter_script_data["ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸"] = st.session_state.get("scene_analysis_script", "")

    chatter_script_source = st.radio(
        "ìŠ¤í¬ë¦½íŠ¸ ì†ŒìŠ¤",
        options=chatter_script_sources,
        horizontal=True,
        key="chatter_script_source"
    )

    # ì”¬ë³„ ìƒì„± ëª¨ë“œ ë³€ìˆ˜ ì´ˆê¸°í™”
    chatter_generation_mode = "single"
    chatter_selected_scenes = []
    script_text = ""

    if chatter_script_source == "ì§ì ‘ ì…ë ¥":
        script_text = st.text_area(
            "í…ìŠ¤íŠ¸ ì…ë ¥",
            value=st.session_state.get("chatter_text_input_value", "ì•ˆë…•í•˜ì„¸ìš”. Chatterbox TTS í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. ìŒì„± í’ˆì§ˆì„ í™•ì¸í•´ë³´ì„¸ìš”."),
            height=150,
            key="chatter_text_input"
        )
    elif chatter_script_source == "ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸" and chatter_has_scene_data:
        # ì”¬ë³„ ìƒì„± ëª¨ë“œ UI
        st.info(f"ğŸ“Š ì´ **{len(chatter_scenes_data)}ê°œ** ì”¬ì´ ë¶„ì„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ìƒì„± ëª¨ë“œ ì„ íƒ
        chatter_generation_mode = st.radio(
            "ğŸ¯ ìƒì„± ëª¨ë“œ",
            options=["ì”¬ë³„ ê°œë³„ ìƒì„±", "ì „ì²´ í•©ì³ì„œ ìƒì„±"],
            horizontal=True,
            key="chatter_generation_mode",
            help="ì”¬ë³„ ê°œë³„ ìƒì„±: ê° ì”¬ë§ˆë‹¤ ë³„ë„ ìŒì„± íŒŒì¼ ìƒì„±\nì „ì²´ í•©ì³ì„œ ìƒì„±: ëª¨ë“  ì”¬ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ìƒì„±"
        )

        st.markdown("---")

        if chatter_generation_mode == "ì”¬ë³„ ê°œë³„ ìƒì„±":
            st.markdown("**ğŸ“‹ ìƒì„±í•  ì”¬ ì„ íƒ**")

            # ì „ì²´ ì„ íƒ/í•´ì œ
            col_sel1, col_sel2 = st.columns([1, 3])
            with col_sel1:
                chatter_select_all = st.checkbox("ì „ì²´ ì„ íƒ", value=True, key="chatter_select_all_scenes")

            # ì”¬ ëª©ë¡ í‘œì‹œ
            chatter_selected_scenes = []
            for idx, scene in enumerate(chatter_scenes_data):
                scene_id = scene.get('scene_id', idx + 1)
                scene_text = scene.get('script_text', '')
                char_count = len(scene_text)
                duration_est = scene.get('duration_estimate', char_count // 10)

                # ì²´í¬ë°•ìŠ¤ì™€ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ê°™ì€ í–‰ì—
                col_check, col_info = st.columns([1, 4])

                with col_check:
                    is_selected = st.checkbox(
                        f"ì”¬ {scene_id}",
                        value=chatter_select_all,
                        key=f"chatter_scene_select_{scene_id}"
                    )

                with col_info:
                    with st.expander(f"{scene_text[:40]}... ({char_count}ì, ~{duration_est}ì´ˆ)", expanded=False):
                        st.text_area(
                            "ë‚´ìš©",
                            value=scene_text,
                            height=100,
                            disabled=True,
                            key=f"chatter_scene_preview_{scene_id}"
                        )

                if is_selected:
                    chatter_selected_scenes.append({
                        "scene_id": scene_id,
                        "text": scene_text,
                        "char_count": char_count,
                        "duration_estimate": duration_est
                    })

            # ì„ íƒ ìš”ì•½
            total_chars = sum(s["char_count"] for s in chatter_selected_scenes)
            st.success(f"âœ… **{len(chatter_selected_scenes)}ê°œ** ì”¬ ì„ íƒë¨ (ì´ {total_chars:,}ì)")

            # ì „ì²´ í…ìŠ¤íŠ¸ (ë¯¸ë¦¬ë³´ê¸°ìš©)
            script_text = "\n\n".join([s["text"] for s in chatter_selected_scenes]) if chatter_selected_scenes else ""

        else:
            # ì „ì²´ í•©ì³ì„œ ìƒì„± ëª¨ë“œ
            full_text = "\n\n".join([s.get('script_text', '') for s in chatter_scenes_data])
            script_text = full_text

            # ë©”íƒ€ ì •ë³´
            total_chars = sum(len(s.get('script_text', '')) for s in chatter_scenes_data)
            total_duration = sum(s.get('duration_estimate', 10) for s in chatter_scenes_data)

            cols = st.columns(3)
            cols[0].metric("ì´ ì”¬ ìˆ˜", f"{len(chatter_scenes_data)}ê°œ")
            cols[1].metric("ì´ ê¸€ì ìˆ˜", f"{total_chars:,}ì")
            cols[2].metric("ì˜ˆìƒ ê¸¸ì´", f"{total_duration // 60}ë¶„ {total_duration % 60}ì´ˆ")

            st.text_area(
                "ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ (ì½ê¸° ì „ìš©)",
                value=full_text,
                height=150,
                disabled=True,
                key="chatter_full_script_preview"
            )

            # ì „ì²´ ì”¬ì„ ì„ íƒëœ ì”¬ìœ¼ë¡œ ì„¤ì •
            chatter_selected_scenes = [{
                "scene_id": s.get('scene_id', idx + 1),
                "text": s.get('script_text', ''),
                "char_count": len(s.get('script_text', '')),
                "duration_estimate": s.get('duration_estimate', 10)
            } for idx, s in enumerate(chatter_scenes_data)]

    elif chatter_script_source in chatter_script_data:
        script_text = chatter_script_data[chatter_script_source]
        st.text_area(f"{chatter_script_source}", value=script_text, height=150, disabled=True, key="chatter_script_preview")
    else:
        script_text = ""
        st.warning("ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë˜ëŠ” ì”¬ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.")

    if script_text:
        st.caption(f"ğŸ“Š {len(script_text)}ì | ì˜ˆìƒ ì‹œê°„: ì•½ {max(1, len(script_text) // 100)}ë¶„")

    st.markdown("---")

    # === ìƒì„± ëª¨ë“œ ì˜µì…˜ (í”„ë¦¬ë·°/ì „ì²´, ì²­í¬ ì„¤ì •) ===
    gen_options = render_chatterbox_generation_options()

    # === ìŒì„± ì •ê·œí™” ì˜µì…˜ (ì”¬ë³„ ì¼ê´€ì„±) ===
    norm_options = render_normalization_options()

    st.markdown("---")

    # === ìƒì„± ë²„íŠ¼ ===
    # ì”¬ë³„ ê°œë³„ ìƒì„± ëª¨ë“œì¼ ë•Œ
    if chatter_script_source == "ì”¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸" and chatter_has_scene_data and chatter_generation_mode == "ì”¬ë³„ ê°œë³„ ìƒì„±":
        # ì”¬ë³„ ìƒì„± - ìƒˆë¡œìš´ robust í•¨ìˆ˜ ì‚¬ìš©
        btn_label = f"ğŸ¤ ì”¬ë³„ ìƒì„± ({len(chatter_selected_scenes)}ê°œ)"
        if gen_options["mode"] == "preview":
            btn_label += " [í”„ë¦¬ë·°]"
        if norm_options["enabled"]:
            btn_label += " + ì •ê·œí™”"

        if st.button(
            btn_label,
            type="primary",
            use_container_width=True,
            disabled=len(chatter_selected_scenes) == 0 or not server_status,
            key="generate_chatterbox_by_scenes"
        ):
            _handle_chatterbox_scenes_generation(
                scenes=chatter_selected_scenes,
                voice_path=voice_path,
                params={
                    "cfg_weight": cfg_weight,
                    "exaggeration": exaggeration,
                    "temperature": temperature,
                    "speed": speed,
                    "seed": seed
                },
                gen_options=gen_options,
                norm_options=norm_options
            )
    else:
        # ì¼ë°˜ ìƒì„± ëª¨ë“œ - í”„ë¦¬ë·°/ì „ì²´ í†µí•©
        btn_label = "ğŸ¤ TTS ìƒì„±"
        if gen_options["mode"] == "preview":
            btn_label = "ğŸ‘ï¸ í”„ë¦¬ë·° ìƒì„±"
        else:
            btn_label = "ğŸ¬ ì „ì²´ TTS ìƒì„±"
        if norm_options["enabled"]:
            btn_label += " + ì •ê·œí™”"

        if st.button(
            btn_label,
            type="primary",
            use_container_width=True,
            disabled=not script_text or not server_status,
            key="generate_chatterbox_robust"
        ):
            _handle_chatterbox_single_generation(
                text=script_text,
                voice_path=voice_path,
                params={
                    "cfg_weight": cfg_weight,
                    "exaggeration": exaggeration,
                    "temperature": temperature,
                    "speed": speed,
                    "seed": seed
                },
                gen_options=gen_options,
                norm_options=norm_options
            )


def _handle_chatterbox_single_generation(text, voice_path, params, gen_options, norm_options=None):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ Chatterbox ìƒì„± í•¸ë“¤ëŸ¬ (ì²­í¬ ë¶„í•  + ì¬ì‹œë„ + ì •ê·œí™”)"""

    if norm_options is None:
        norm_options = {"enabled": False}

    result_container = st.container()

    with result_container:
        progress_bar = st.progress(0, text="ì¤€ë¹„ ì¤‘...")
        status_text = st.empty()

        voice_name = os.path.basename(voice_path) if voice_path else "ê¸°ë³¸ ìŒì„±"
        mode_label = "í”„ë¦¬ë·°" if gen_options["mode"] == "preview" else "ì „ì²´"
        norm_label = " + ì •ê·œí™”" if norm_options.get("enabled") else ""

        status_text.info(f"ğŸ™ï¸ {mode_label}{norm_label} TTS ìƒì„± ì¤€ë¹„ ì¤‘... (ì°¸ì¡° ìŒì„±: {voice_name})")

        # â­ ì°¸ì¡° ìŒì„± ìµœì í™” (ê¸´ ìŒì„± â†’ 15~30ì´ˆ ì¶”ì¶œ)
        optimized_voice_path = voice_path
        if voice_path:
            try:
                from pydub import AudioSegment
                voice_audio = AudioSegment.from_file(voice_path)
                voice_duration = len(voice_audio) / 1000

                if voice_duration > 60:  # 60ì´ˆ ì´ìƒì´ë©´ ìµœì í™”
                    status_text.text(f"ğŸ” ì°¸ì¡° ìŒì„± ìµœì í™” ì¤‘... ({voice_duration:.0f}ì´ˆ â†’ 20ì´ˆ)")
                    optimized_voice_path = optimize_voice_for_cloning(voice_path)

                    if optimized_voice_path != voice_path:
                        opt_audio = AudioSegment.from_file(optimized_voice_path)
                        print(f"[VoiceOptimizer] ìµœì í™” ì ìš©: {voice_duration:.0f}ì´ˆ â†’ {len(opt_audio)/1000:.0f}ì´ˆ")
            except Exception as e:
                print(f"[VoiceOptimizer] ìµœì í™” ì‹¤íŒ¨: {e}")
                optimized_voice_path = voice_path

        def progress_callback(current, total, message):
            if total > 0:
                progress_bar.progress((current / total), text=message)
            status_text.text(message)

        # Robust ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
        result = generate_chatterbox_tts_robust(
            text=text,
            voice_ref_path=optimized_voice_path,  # â­ ìµœì í™”ëœ ìŒì„± ì‚¬ìš©
            params=params,
            mode=gen_options["mode"],
            preview_length=gen_options["preview_length"],
            chunk_size=gen_options["chunk_size"],
            repetition_penalty=gen_options["repetition_penalty"],
            max_retries=gen_options["max_retries"],
            pause_ms=gen_options["pause_ms"],
            progress_callback=progress_callback
        )

        # ì •ê·œí™” ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
        if result.get("success") and norm_options.get("enabled"):
            progress_bar.progress(0.9, text="ìŒì„± ì •ê·œí™” ì¤‘...")
            status_text.text("ğŸšï¸ ìŒì„± ì •ê·œí™” ì ìš© ì¤‘...")
            result = apply_normalization_to_result(result, text, norm_options)

        progress_bar.progress(1.0, text="ì™„ë£Œ!")

        if result.get("success"):
            stats = result.get("stats", {})
            mode_info = "í”„ë¦¬ë·°" if result.get("mode") == "preview" else "ì „ì²´"
            norm_info = " (ì •ê·œí™”ë¨)" if result.get("normalized") else ""

            # ì„±ê³µ ë©”ì‹œì§€
            if stats.get("truncated_count", 0) > 0:
                status_text.warning(
                    f"âš ï¸ {mode_info} ìƒì„± ì™„ë£Œ{norm_info} (ì¼ë¶€ ì˜ë¦¼ ê²½ê³ )\n"
                    f"ì²­í¬: {stats['success_count']}/{stats['total_chunks']}ê°œ ì„±ê³µ, "
                    f"ê¸¸ì´: {result.get('final_duration', result['duration']):.1f}ì´ˆ"
                )
            else:
                status_text.success(
                    f"âœ… {mode_info} ìƒì„± ì™„ë£Œ{norm_info}! "
                    f"ì²­í¬: {stats['success_count']}/{stats['total_chunks']}ê°œ, "
                    f"ê¸¸ì´: {result.get('final_duration', result['duration']):.1f}ì´ˆ"
                )

            # ì˜¤ë””ì˜¤ ì¬ìƒ
            if result.get("audio_data"):
                st.audio(result["audio_data"], format="audio/wav")

                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                timestamp = int(time.time())
                st.download_button(
                    "â¬‡ï¸ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ (WAV)",
                    data=result["audio_data"],
                    file_name=f"chatterbox_{mode_info}_{timestamp}.wav",
                    mime="audio/wav",
                    key=f"download_chatterbox_robust_{timestamp}"
                )

            # ì²­í¬ë³„ ìƒì„¸ ì •ë³´
            with st.expander("ğŸ“Š ìƒì„± ìƒì„¸ ì •ë³´", expanded=False):
                st.json({
                    "mode": result.get("mode"),
                    "original_length": result.get("original_length"),
                    "processed_length": result.get("processed_length"),
                    "total_duration": f"{result['duration']:.2f}ì´ˆ",
                    "chunks": stats
                })

                # ì²­í¬ë³„ ìƒíƒœ
                if result.get("chunks_info"):
                    st.markdown("**ì²­í¬ë³„ ê²°ê³¼:**")
                    for chunk in result["chunks_info"]:
                        if chunk.get("status") == "success":
                            st.success(f"âœ… ì²­í¬ {chunk['index']}: {chunk['text_preview']} ({chunk['duration']:.1f}ì´ˆ)")
                        elif chunk.get("status") == "truncated":
                            st.warning(f"âš ï¸ ì²­í¬ {chunk['index']}: {chunk['text_preview']} - {chunk.get('warning', 'ì˜ë¦¼')}")
                        else:
                            st.error(f"âŒ ì²­í¬ {chunk['index']}: {chunk.get('error', 'ì‹¤íŒ¨')}")
        else:
            status_text.error(f"âŒ ìƒì„± ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

            # ì‹¤íŒ¨ ìƒì„¸ ì •ë³´
            if result.get("chunks_info"):
                with st.expander("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                    for chunk in result["chunks_info"]:
                        if chunk.get("status") == "failed":
                            st.error(f"ì²­í¬ {chunk['index']}: {chunk.get('error', 'ì‹¤íŒ¨')}")


def _handle_chatterbox_scenes_generation(scenes, voice_path, params, gen_options, norm_options=None):
    """ì”¬ë³„ Chatterbox ìƒì„± í•¸ë“¤ëŸ¬ (ìˆœì°¨/ë³‘ë ¬ + ì²­í¬ ë¶„í•  + ì¬ì‹œë„ + ì •ë°€ ì •ê·œí™”)"""

    if norm_options is None:
        norm_options = {"enabled": False}

    progress_bar = st.progress(0)
    status_text = st.empty()
    time_display = st.empty()
    results_container = st.container()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = st.session_state.get("tts_output_dir", "data/tts")
    timestamp = int(time.time())
    scene_output_dir = os.path.join(output_dir, f"chatterbox_scenes_{timestamp}")
    os.makedirs(scene_output_dir, exist_ok=True)

    generated_files = []
    total_scenes = len(scenes)

    # ğŸ”§ Seed ì¼ê´€ì„± ë³´ì¥: ì „ì²´ ì”¬ì— ë™ì¼í•œ seed ì‚¬ìš©
    import random
    scene_seed = params.get("seed")
    if scene_seed is None:
        scene_seed = random.randint(0, 2**31 - 1)
        print(f"[TTS] ì”¬ë³„ ìƒì„±: ëœë¤ seed ê³ ì • â†’ {scene_seed}")
    else:
        print(f"[TTS] ì”¬ë³„ ìƒì„±: ì‚¬ìš©ì ì§€ì • seed â†’ {scene_seed}")

    # params ë³µì‚¬ë³¸ì— ê³ ì • seed ì ìš©
    scene_params = params.copy()
    scene_params["seed"] = scene_seed

    # â­ ì°¸ì¡° ìŒì„± ìµœì í™” (ê¸´ ìŒì„± â†’ 15~30ì´ˆ ì¶”ì¶œ)
    optimized_voice_path = voice_path
    if voice_path:
        try:
            from pydub import AudioSegment
            voice_audio = AudioSegment.from_file(voice_path)
            voice_duration = len(voice_audio) / 1000

            if voice_duration > 60:  # 60ì´ˆ ì´ìƒì´ë©´ ìµœì í™”
                status_text.text(f"ğŸ” ì°¸ì¡° ìŒì„± ìµœì í™” ì¤‘... ({voice_duration:.0f}ì´ˆ â†’ 20ì´ˆ)")
                optimized_voice_path = optimize_voice_for_cloning(voice_path)

                if optimized_voice_path != voice_path:
                    opt_audio = AudioSegment.from_file(optimized_voice_path)
                    print(f"[VoiceOptimizer] ì”¬ë³„ ìƒì„±: ìµœì í™” ì ìš© {voice_duration:.0f}ì´ˆ â†’ {len(opt_audio)/1000:.0f}ì´ˆ")
        except Exception as e:
            print(f"[VoiceOptimizer] ì”¬ë³„ ìƒì„±: ìµœì í™” ì‹¤íŒ¨ - {e}")
            optimized_voice_path = voice_path

    scene_params["voice_ref_path"] = optimized_voice_path  # â­ ìµœì í™”ëœ ìŒì„± ì‚¬ìš©

    # ì²˜ë¦¬ ë°©ì‹ ì˜µì…˜ í™•ì¸
    use_sequential = gen_options.get("use_sequential", True)
    use_smart_chunking = gen_options.get("use_smart_chunking", True)
    timeout_per_scene = gen_options.get("timeout_per_scene", 180)
    max_concurrent = gen_options.get("max_concurrent", 2)
    chunk_size = gen_options.get("chunk_size", 70)

    voice_info = os.path.basename(voice_path) if voice_path else "ê¸°ë³¸ ìŒì„±"
    total_start = time.time()

    # ì”¬ ë°ì´í„° ì¤€ë¹„
    scene_list = [
        {"scene_id": s.get("scene_id", idx + 1), "text": s.get("text", "")}
        for idx, s in enumerate(scenes)
        if s.get("text", "").strip()
    ]

    if use_sequential:
        # ============================================================
        # ğŸš€ ë³‘ë ¬ ìƒì„± ëª¨ë“œ (ë™ì‹œ 3ê°œ ì²˜ë¦¬ - 40% ì†ë„ í–¥ìƒ!)
        # ============================================================
        parallel_workers = 4  # RTX 5070 + FP16 ìµœì í™”ë¡œ 4ê°œ ë™ì‹œ ì²˜ë¦¬
        mode_label = f"ğŸš€ ë³‘ë ¬ ìƒì„± (ë™ì‹œ {parallel_workers}ê°œ)"
        status_text.info(f"{mode_label} ({total_scenes}ê°œ ì”¬) - {voice_info}")
        print(f"\n[TTS] ğŸš€ ë³‘ë ¬ ìƒì„± ëª¨ë“œ - ë™ì‹œ {parallel_workers}ê°œ ì²˜ë¦¬!")

        def parallel_gen_progress(current, total, message):
            progress = current / total * (0.8 if norm_options.get("enabled") else 1.0)
            progress_bar.progress(min(progress, 1.0))
            status_text.text(f"ğŸš€ {message}")
            elapsed = time.time() - total_start
            time_display.text(f"â±ï¸ ê²½ê³¼: {elapsed:.0f}ì´ˆ")

        try:
            # ğŸ¯ ìˆœì°¨ ìƒì„± ëª¨ë“œ (GPU 1ê°œ í™˜ê²½ ìµœì í™”)
            # ë³‘ë ¬ì€ ì„œë²„ì—ì„œ íì‰ë˜ì–´ ì‹¤ì œë¡œëŠ” ìˆœì°¨ + ì˜¤ë²„í—¤ë“œ
            generated_files = generate_scenes_parallel(
                scenes=scene_list,
                params=scene_params,
                max_workers=parallel_workers,
                timeout_per_scene=timeout_per_scene,
                use_sequential=True,  # â­ ìˆœì°¨ ëª¨ë“œ (GPU 1ê°œ ìµœì )
                progress_callback=parallel_gen_progress
            )

            gen_time = time.time() - total_start
            success_count = sum(1 for f in generated_files if f.get("success") and f.get("audio_data"))
            print(f"[TTS] ğŸ¯ ìˆœì°¨ ìƒì„± ì™„ë£Œ: {success_count}/{total_scenes}ê°œ, {gen_time:.1f}ì´ˆ")
            print(f"[TTS] ì”¬ë‹¹ í‰ê· : {gen_time/total_scenes:.1f}ì´ˆ")

        except Exception as e:
            print(f"[TTS] ë³‘ë ¬ ìƒì„± ì˜¤ë¥˜: {e}")
            st.error(f"ìƒì„± ì˜¤ë¥˜: {e}")
            generated_files = []

    else:
        # ============================================================
        # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ (ë¹ ë¥´ì§€ë§Œ íƒ€ì„ì•„ì›ƒ ìœ„í—˜)
        # ============================================================
        status_text.info(f"ğŸš€ ë³‘ë ¬ ìƒì„± ì‹œì‘ ({total_scenes}ê°œ ì”¬, ë™ì‹œ {max_concurrent}ê°œ) - {voice_info}")

        def parallel_progress(current, total, message):
            progress = current / total * (0.8 if norm_options.get("enabled") else 1.0)
            progress_bar.progress(min(progress, 1.0))
            status_text.text(f"ğŸš€ {message} ({current}/{total})")
            elapsed = time.time() - total_start
            time_display.text(f"â±ï¸ ê²½ê³¼: {elapsed:.0f}ì´ˆ")

        try:
            parallel_results = run_threaded_generation(
                scenes=scene_list,
                params=scene_params,
                max_workers=max_concurrent,
                progress_callback=parallel_progress
            )

            # ê²°ê³¼ ë³€í™˜
            for pr in parallel_results:
                scene_id = pr.get("scene_id", 0)
                scene_text = next((s["text"] for s in scene_list if s["scene_id"] == scene_id), "")

                if pr.get("success"):
                    generated_files.append({
                        "scene_id": scene_id,
                        "audio_data": pr.get("audio_data"),
                        "text": scene_text,
                        "text_preview": scene_text[:50] + "..." if len(scene_text) > 50 else scene_text,
                        "char_count": len(scene_text),
                        "duration": pr.get("duration", 0),
                        "chunks_count": 1,
                        "status": "success",
                        "success": True
                    })
                else:
                    generated_files.append({
                        "scene_id": scene_id,
                        "audio_data": None,
                        "text": scene_text,
                        "error": pr.get("error", "ìƒì„± ì‹¤íŒ¨"),
                        "status": "failed",
                        "success": False
                    })

            gen_time = time.time() - total_start
            success_count = sum(1 for f in generated_files if f.get("success"))
            print(f"[TTS] ë³‘ë ¬ ìƒì„± ì™„ë£Œ: {success_count}/{total_scenes}ê°œ, {gen_time:.1f}ì´ˆ")

        except Exception as e:
            print(f"[TTS] ë³‘ë ¬ ìƒì„± ì‹¤íŒ¨: {e}")
            st.error(f"ë³‘ë ¬ ìƒì„± ì˜¤ë¥˜: {e}")
            generated_files = []

    gen_time = time.time() - total_start

    # ============================================================
    # â­ í†µí•© ë‹¨ì¼ íŒ¨ìŠ¤ ì²˜ë¦¬ (ì •ê·œí™” + ê°€ì†ë³´ì • + ë¯¸ì„¸ì¡°ì •)
    # ============================================================
    # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ (ë¬¸ì œ):
    #   1ë‹¨ê³„: normalize_perfect (FFmpeg 2~3íšŒ)
    #   2ë‹¨ê³„: correct_all_speed_acceleration (FFmpeg 4íšŒ)
    #   3ë‹¨ê³„: normalize_segments_all (FFmpeg 1~2íšŒ)
    #   â†’ ì´ 6~9íšŒ FFmpeg â†’ ìš¸ë¦¼, ë³€ì¡°, í’ˆì§ˆ ì €í•˜
    #
    # ìƒˆ íŒŒì´í”„ë¼ì¸ (í•´ê²°):
    #   process_all_unified (êµ¬ê°„ë‹¹ FFmpeg 1íšŒ)
    #   â†’ í’ˆì§ˆ ìœ ì§€, ìš¸ë¦¼ ì—†ìŒ
    # ============================================================
    if norm_options.get("enabled") and generated_files:
        status_text.text("ğŸ”§ í†µí•© ì²˜ë¦¬ ì¤‘... (ì •ê·œí™” + ê°€ì†ë³´ì • + ë¯¸ì„¸ì¡°ì •)")
        print("\n" + "="*60)
        print("[TTS] â­ í†µí•© ë‹¨ì¼ íŒ¨ìŠ¤ ì²˜ë¦¬ ì‹œì‘")
        print("[TTS] â­ FFmpeg ìµœì†Œ í˜¸ì¶œ â†’ ìš¸ë¦¼/ë³€ì¡° ë°©ì§€")
        print("[TTS] â­ ì ì‘í˜• ê°€ì† ë³´ì • â†’ ì •í™•í•œ ì†ë„ ê· ì¼í™”")
        print("="*60)

        # ì²˜ë¦¬ ì „ ìƒíƒœ ë¶„ì„
        pre_stats = analyze_normalization_stats(generated_files)
        if not pre_stats.get("error"):
            print(f"[TTS] ì²˜ë¦¬ ì „ ë°œí™”ì†ë„: {pre_stats['rate_min']:.2f} ~ {pre_stats['rate_max']:.2f} (Â±{pre_stats['rate_deviation_pct']:.1f}%)")

        def unified_progress(current, total, message):
            base_progress = 0.75
            step = (current / total) * 0.23  # 0.75 ~ 0.98
            progress_bar.progress(min(base_progress + step, 0.98))
            status_text.text(f"ğŸ”§ {message}")

        # â­ í†µí•© ë‹¨ì¼ íŒ¨ìŠ¤ ì²˜ë¦¬
        # ì°¸ì¡° ìŒì„± ë¶„ì„ ê²°ê³¼ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
        target_speed = st.session_state.get("target_speech_rate", 8.5)
        print(f"[TTS] ëª©í‘œ ë°œí™”ì†ë„: {target_speed:.2f} ê¸€ì/ì´ˆ (ì°¸ì¡° ìŒì„± ê¸°ë°˜)")

        generated_files = process_all_unified(
            generated_files,
            target_speed=target_speed,  # â­ ì°¸ì¡° ìŒì„± ê¸°ë°˜ ëª©í‘œ
            accel_profile="adaptive",   # ì ì‘í˜• ê°€ì† ë³´ì •
            progress_callback=unified_progress
        )

        # ì²˜ë¦¬ í›„ ìƒíƒœ í™•ì¸
        post_stats = analyze_normalization_stats(generated_files)
        if not post_stats.get("error"):
            print(f"[TTS] ì²˜ë¦¬ í›„ ë°œí™”ì†ë„: {post_stats['rate_min']:.2f} ~ {post_stats['rate_max']:.2f} (Â±{post_stats['rate_deviation_pct']:.1f}%)")
            improvement = pre_stats.get('rate_deviation_pct', 0) - post_stats.get('rate_deviation_pct', 0)
            if improvement > 0:
                print(f"[TTS] âœ… í¸ì°¨ ê°œì„ : {improvement:.1f}% ê°ì†Œ")

        print("[TTS] í†µí•© ì²˜ë¦¬ ì™„ë£Œ")
        print("="*60 + "\n")

    total_time = time.time() - total_start
    time_display.text(f"â±ï¸ ì´ {total_time:.1f}ì´ˆ (ìƒì„±: {gen_time:.1f}ì´ˆ)")

    # íŒŒì¼ ì €ì¥ (ì •ê·œí™” í›„)
    for file_info in generated_files:
        if file_info.get("audio_data") and file_info["status"] in ["success", "partial"]:
            audio_path = os.path.join(scene_output_dir, f"scene_{file_info['scene_id']:02d}.wav")
            with open(audio_path, "wb") as f:
                f.write(file_info["audio_data"])
            file_info["path"] = audio_path

    progress_bar.progress(1.0)
    status_text.empty()

    # ê²°ê³¼ í‘œì‹œ (ì•ˆì „í•œ ì ‘ê·¼)
    # â­ í•µì‹¬: success=True AND audio_data ìˆì–´ì•¼ ì„±ê³µ
    success_count = len([
        f for f in generated_files
        if f and f.get("success") == True and f.get("audio_data")
    ])
    failed_count = len([
        f for f in generated_files
        if f and (not f.get("success") or not f.get("audio_data"))
    ])
    normalized_count = len([f for f in generated_files if f and f.get("normalized")])

    # ë””ë²„ê·¸ ë¡œê¹…
    print(f"\n[TTS ê²°ê³¼] ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {failed_count}")
    for idx, f in enumerate(generated_files):
        if f:
            has_audio = "O" if f.get("audio_data") else "X"
            print(f"  [{idx+1}] success={f.get('success')}, audio={has_audio}, status={f.get('status')}")

    with results_container:
        if success_count > 0:
            norm_info = f" (ì •ê·œí™”: {normalized_count}ê°œ)" if norm_options.get("enabled") else ""
            st.success(f"âœ… **{success_count}/{total_scenes}ê°œ** ì”¬ ìƒì„± ì™„ë£Œ!{norm_info}")

            st.markdown("### ğŸµ ìƒì„±ëœ ìŒì„± íŒŒì¼")

            for file_info in generated_files:
                if not file_info:
                    continue
                scene_id = file_info.get("scene_id", 0)
                file_status = file_info.get("status", "")
                has_audio = file_info.get("audio_data") is not None
                is_success = file_info.get("success") == True and has_audio

                if is_success or file_status in ["success", "partial"]:
                    status_icon = "âœ…" if is_success else "âš ï¸"
                    text_preview = file_info.get("text_preview", file_info.get("text", "")[:50])
                    char_count = file_info.get("char_count", len(file_info.get("text", "")))
                    with st.expander(f"{status_icon} ì”¬ {scene_id} - {text_preview} ({char_count}ì)", expanded=True):
                        col1, col2 = st.columns([3, 1])

                        with col1:
                            if file_info.get("audio_data"):
                                st.audio(file_info["audio_data"], format="audio/wav")
                            elif file_info.get("path"):
                                st.audio(file_info["path"])
                            st.caption(f"â±ï¸ {file_info.get('duration', 0):.1f}ì´ˆ | ì²­í¬: {file_info.get('chunks_count', 1)}ê°œ")

                        with col2:
                            if file_info.get("audio_data"):
                                st.download_button(
                                    "â¬‡ï¸ ë‹¤ìš´ë¡œë“œ",
                                    data=file_info["audio_data"],
                                    file_name=f"scene_{scene_id:02d}.wav",
                                    mime="audio/wav",
                                    key=f"chatter_robust_dl_scene_{scene_id}_{timestamp}",
                                    use_container_width=True
                                )
                else:
                    st.error(f"âŒ ì”¬ {scene_id} ìƒì„± ì‹¤íŒ¨: {file_info.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

            # ì „ì²´ ZIP ë‹¤ìš´ë¡œë“œ
            if success_count > 1:
                st.markdown("---")
                st.markdown("### ğŸ“¦ ì¼ê´„ ë‹¤ìš´ë¡œë“œ")

                import zipfile

                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for file_info in generated_files:
                        if file_info["status"] in ["success", "partial"] and file_info.get("audio_data"):
                            scene_id = file_info["scene_id"]
                            zip_file.writestr(f"scene_{scene_id:02d}.wav", file_info["audio_data"])

                zip_buffer.seek(0)

                st.download_button(
                    f"ğŸ“¦ ì „ì²´ ë‹¤ìš´ë¡œë“œ (ZIP, {success_count}ê°œ íŒŒì¼)",
                    data=zip_buffer.getvalue(),
                    file_name=f"chatterbox_scenes_{timestamp}.zip",
                    mime="application/zip",
                    key=f"chatter_robust_dl_all_zip_{timestamp}",
                    use_container_width=True
                )

            # ì„¸ì…˜ì— ì €ì¥
            st.session_state["last_chatterbox_scenes"] = generated_files
            st.session_state["last_chatterbox_output_dir"] = scene_output_dir

        if failed_count > 0:
            st.warning(f"âš ï¸ {failed_count}ê°œ ì”¬ ìƒì„± ì‹¤íŒ¨")


def generate_chatterbox_tts(text, cfg_weight, exaggeration, temperature, speed, seed, voice_ref_path):
    """Chatterbox TTS ìƒì„± (ê°œì„ ëœ ë¡œë”© UI)"""

    # ê²°ê³¼ í‘œì‹œ ì˜ì—­
    result_container = st.container()

    with result_container:
        # ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_bar = st.progress(0, text="ì¤€ë¹„ ì¤‘...")
        status_text = st.empty()

        # ì°¸ì¡° ìŒì„± ì •ë³´
        voice_name = os.path.basename(voice_ref_path) if voice_ref_path else "ê¸°ë³¸ ìŒì„±"

        # 1ë‹¨ê³„: ì¤€ë¹„
        status_text.info(f"ğŸ™ï¸ TTS ìƒì„± ì¤€ë¹„ ì¤‘... (ì°¸ì¡° ìŒì„±: {voice_name})")
        progress_bar.progress(10, text="ì„œë²„ ì—°ê²° í™•ì¸...")

        try:
            # 2ë‹¨ê³„: ì„œë²„ ìƒíƒœ í™•ì¸ (ìºì‹œ ì‚¬ìš©)
            if not check_chatterbox_server():
                status_text.error("âŒ Chatterbox ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                progress_bar.empty()
                return

            progress_bar.progress(20, text="ìš”ì²­ ì¤€ë¹„ ì¤‘...")

            payload = {
                "text": text,
                "settings": {
                    "language": "ko",
                    "exaggeration": exaggeration,
                    "cfg_weight": cfg_weight,
                    "temperature": temperature,
                    "speed": speed,
                    "seed": seed,
                    "voice_ref_path": voice_ref_path
                }
            }

            # 3ë‹¨ê³„: TTS ìƒì„± ìš”ì²­
            progress_bar.progress(30, text="ğŸ¤ ìŒì„± ìƒì„± ì¤‘... (10~30ì´ˆ ì†Œìš”)")
            status_text.info(f"ğŸ”„ Chatterboxì—ì„œ ìŒì„± ìƒì„± ì¤‘... ({len(text)}ì)")

            start_time = time.time()
            r = requests.post(f"{CHATTERBOX_URL}/generate", json=payload, timeout=180)
            elapsed = time.time() - start_time

            # 4ë‹¨ê³„: ì‘ë‹µ ì²˜ë¦¬
            progress_bar.progress(80, text="ì‘ë‹µ ì²˜ë¦¬ ì¤‘...")

            if r.status_code == 200:
                result = r.json()

                if result.get("success"):
                    progress_bar.progress(90, text="ì˜¤ë””ì˜¤ ë¡œë”© ì¤‘...")

                    # ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
                    audio_url = result.get("audio_url", "")
                    audio_data = None

                    if audio_url:
                        full_url = f"{CHATTERBOX_URL}{audio_url}"
                        try:
                            audio_response = requests.get(full_url, timeout=30)
                            if audio_response.status_code == 200:
                                audio_data = audio_response.content
                        except Exception as e:
                            st.warning(f"ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

                    # ì™„ë£Œ!
                    progress_bar.progress(100, text="ì™„ë£Œ!")
                    voice_info = f"ğŸ¤ {voice_name}"
                    status_text.success(f"âœ… ìƒì„± ì™„ë£Œ! (ê¸¸ì´: {result.get('duration_seconds', 0):.1f}ì´ˆ, ì²˜ë¦¬ì‹œê°„: {elapsed:.1f}s, {voice_info})")

                    # ì˜¤ë””ì˜¤ ì¬ìƒ
                    if audio_data:
                        st.audio(audio_data, format="audio/wav")

                        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        timestamp = int(time.time())
                        st.download_button(
                            "â¬‡ï¸ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ (WAV)",
                            data=audio_data,
                            file_name=f"chatterbox_{timestamp}.wav",
                            mime="audio/wav",
                            key=f"download_chatterbox_{timestamp}"
                        )

                    # ìƒì„± ì •ë³´
                    with st.expander("ğŸ“Š ìƒì„± ì •ë³´", expanded=False):
                        st.json({
                            "duration_seconds": result.get("duration_seconds"),
                            "seed_used": result.get("seed_used"),
                            "processing_time": f"{elapsed:.2f}s",
                            "text_length": len(text),
                            "voice_ref": voice_name
                        })
                else:
                    progress_bar.empty()
                    status_text.error(f"âŒ ìƒì„± ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            else:
                progress_bar.empty()
                status_text.error(f"âŒ ì„œë²„ ì˜¤ë¥˜: HTTP {r.status_code}")

        except requests.exceptions.Timeout:
            progress_bar.empty()
            status_text.error("â±ï¸ ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (180ì´ˆ). í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì„œë²„ê°€ ë°”ì  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        except requests.exceptions.ConnectionError:
            progress_bar.empty()
            status_text.error("ğŸ”Œ ì„œë²„ ì—°ê²° ì‹¤íŒ¨. Chatterbox ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            invalidate_chatterbox_cache()  # ìºì‹œ ë¬´íš¨í™”
        except Exception as e:
            progress_bar.empty()
            status_text.error(f"âŒ ì˜¤ë¥˜: {e}")
            import traceback
            with st.expander("ìƒì„¸ ì˜¤ë¥˜"):
                st.code(traceback.format_exc())


def generate_chatterbox_tts_by_scenes(scenes, cfg_weight, exaggeration, temperature, speed, seed, voice_ref_path):
    """ì”¬ë³„ Chatterbox TTS ê°œë³„ ìƒì„±"""

    # ì§„í–‰ ìƒí™© UI
    progress_bar = st.progress(0)
    status_text = st.empty()
    results_container = st.container()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = st.session_state.get("tts_output_dir", "data/tts")
    timestamp = int(time.time())
    scene_output_dir = os.path.join(output_dir, f"chatterbox_scenes_{timestamp}")
    os.makedirs(scene_output_dir, exist_ok=True)

    generated_files = []
    total_scenes = len(scenes)

    try:
        # ì”¬ë³„ ìƒì„± ë£¨í”„
        for idx, scene in enumerate(scenes):
            scene_id = scene.get("scene_id", idx + 1)
            scene_text = scene.get("text", "")

            if not scene_text.strip():
                continue

            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            progress_bar.progress((idx + 1) / total_scenes)
            voice_info = os.path.basename(voice_ref_path) if voice_ref_path else "ê¸°ë³¸ ìŒì„±"
            status_text.text(f"ì”¬ {scene_id} ìƒì„± ì¤‘... ({idx + 1}/{total_scenes}) - {voice_info}")

            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            audio_path = os.path.join(scene_output_dir, f"scene_{scene_id:02d}.wav")

            try:
                # Chatterbox TTS API í˜¸ì¶œ
                payload = {
                    "text": scene_text,
                    "settings": {
                        "language": "ko",
                        "exaggeration": exaggeration,
                        "cfg_weight": cfg_weight,
                        "temperature": temperature,
                        "speed": speed,
                        "seed": seed,
                        "voice_ref_path": voice_ref_path
                    }
                }

                start_time = time.time()
                r = requests.post(f"{CHATTERBOX_URL}/generate", json=payload, timeout=120)
                elapsed = time.time() - start_time

                if r.status_code == 200:
                    result = r.json()

                    if result.get("success"):
                        # ì˜¤ë””ì˜¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
                        audio_url = result.get("audio_url", "")
                        if audio_url:
                            full_url = f"{CHATTERBOX_URL}{audio_url}"
                            audio_response = requests.get(full_url, timeout=30)
                            if audio_response.status_code == 200:
                                with open(audio_path, "wb") as f:
                                    f.write(audio_response.content)

                                generated_files.append({
                                    "scene_id": scene_id,
                                    "path": audio_path,
                                    "text_preview": scene_text[:50] + "..." if len(scene_text) > 50 else scene_text,
                                    "char_count": len(scene_text),
                                    "duration": result.get("duration_seconds", 0),
                                    "processing_time": elapsed,
                                    "status": "success"
                                })
                            else:
                                raise Exception(f"ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {audio_response.status_code}")
                        else:
                            raise Exception("ì˜¤ë””ì˜¤ URLì´ ì—†ìŠµë‹ˆë‹¤")
                    else:
                        raise Exception(result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"))
                else:
                    raise Exception(f"ì„œë²„ ì˜¤ë¥˜: {r.status_code}")

            except requests.exceptions.ConnectionError:
                # ì—°ê²° ì˜¤ë¥˜ ì‹œ ìºì‹œ ë¬´íš¨í™”
                invalidate_chatterbox_cache()
                generated_files.append({
                    "scene_id": scene_id,
                    "path": None,
                    "error": "ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - Chatterbox ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”",
                    "status": "failed"
                })
                # ì—°ê²° ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‚˜ë¨¸ì§€ ì”¬ë„ ì‹¤íŒ¨í•  ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì¤‘ë‹¨
                status_text.error("ğŸ”Œ ì„œë²„ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ìƒì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            except requests.exceptions.Timeout:
                generated_files.append({
                    "scene_id": scene_id,
                    "path": None,
                    "error": "ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (120ì´ˆ)",
                    "status": "failed"
                })
            except Exception as e:
                generated_files.append({
                    "scene_id": scene_id,
                    "path": None,
                    "error": str(e),
                    "status": "failed"
                })

        progress_bar.progress(1.0)
        status_text.empty()

        # ê²°ê³¼ í‘œì‹œ
        success_count = len([f for f in generated_files if f["status"] == "success"])
        failed_count = len([f for f in generated_files if f["status"] == "failed"])

        with results_container:
            if success_count > 0:
                st.success(f"âœ… **{success_count}/{total_scenes}ê°œ** ì”¬ ìƒì„± ì™„ë£Œ!")

                # ì”¬ë³„ ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ë° ë‹¤ìš´ë¡œë“œ
                st.markdown("### ğŸµ ìƒì„±ëœ ìŒì„± íŒŒì¼")

                for file_info in generated_files:
                    scene_id = file_info["scene_id"]

                    if file_info["status"] == "success":
                        with st.expander(f"ğŸ“¢ ì”¬ {scene_id} - {file_info['text_preview']} ({file_info['char_count']}ì)", expanded=True):
                            col1, col2 = st.columns([3, 1])

                            with col1:
                                st.audio(file_info["path"])
                                st.caption(f"â±ï¸ {file_info.get('duration', 0):.1f}ì´ˆ | ì²˜ë¦¬ì‹œê°„: {file_info.get('processing_time', 0):.1f}s")

                            with col2:
                                with open(file_info["path"], "rb") as f:
                                    st.download_button(
                                        "â¬‡ï¸ ë‹¤ìš´ë¡œë“œ",
                                        data=f.read(),
                                        file_name=f"scene_{scene_id:02d}.wav",
                                        mime="audio/wav",
                                        key=f"chatter_download_scene_{scene_id}_{timestamp}",
                                        use_container_width=True
                                    )
                    else:
                        st.error(f"âŒ ì”¬ {scene_id} ìƒì„± ì‹¤íŒ¨: {file_info.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

                # ì „ì²´ ZIP ë‹¤ìš´ë¡œë“œ
                if success_count > 1:
                    st.markdown("---")
                    st.markdown("### ğŸ“¦ ì¼ê´„ ë‹¤ìš´ë¡œë“œ")

                    # ZIP íŒŒì¼ ìƒì„±
                    import zipfile
                    import io

                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for file_info in generated_files:
                            if file_info["status"] == "success" and file_info["path"]:
                                scene_id = file_info["scene_id"]
                                zip_file.write(file_info["path"], f"scene_{scene_id:02d}.wav")

                    zip_buffer.seek(0)

                    st.download_button(
                        f"ğŸ“¦ ì „ì²´ ë‹¤ìš´ë¡œë“œ (ZIP, {success_count}ê°œ íŒŒì¼)",
                        data=zip_buffer.getvalue(),
                        file_name=f"chatterbox_scenes_{timestamp}.zip",
                        mime="application/zip",
                        key=f"chatter_download_all_zip_{timestamp}",
                        use_container_width=True
                    )

                # ì„¸ì…˜ì— ì €ì¥
                st.session_state["last_chatterbox_scenes"] = generated_files
                st.session_state["last_chatterbox_output_dir"] = scene_output_dir

            if failed_count > 0:
                st.warning(f"âš ï¸ {failed_count}ê°œ ì”¬ ìƒì„± ì‹¤íŒ¨")

    except Exception as e:
        status_text.error(f"ì˜¤ë¥˜: {e}")
        import traceback
        with st.expander("ìƒì„¸ ì˜¤ë¥˜"):
            st.code(traceback.format_exc())


# ============================================================
# ìˆ˜ë™ ì…ë ¥ íƒ­
# ============================================================

def render_manual_input_tab():
    """ìˆ˜ë™ ì…ë ¥ íƒ­"""
    st.markdown("### âœï¸ ìˆ˜ë™ ì…ë ¥")
    st.info("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ",
        type=["txt", "md"],
        key="manual_upload"
    )

    if uploaded_file:
        content = uploaded_file.read().decode("utf-8")
        st.session_state["manual_script"] = content
        st.success(f"íŒŒì¼ ë¡œë“œ: {len(content)}ì")

    # í…ìŠ¤íŠ¸ ì…ë ¥
    script = st.text_area(
        "ìŠ¤í¬ë¦½íŠ¸",
        value=st.session_state.get("manual_script", ""),
        height=400,
        placeholder="TTSë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
        key="manual_script_input"
    )

    if script:
        st.caption(f"ğŸ“Š {len(script)}ì | {len(script.split())}ë‹¨ì–´ | {script.count(chr(10)) + 1}ì¤„")
        st.session_state["generated_script"] = script

    # ì €ì¥ ë²„íŠ¼
    if st.button("ğŸ’¾ ìŠ¤í¬ë¦½íŠ¸ ì €ì¥", use_container_width=True, key="save_manual_script"):
        if script:
            st.session_state["generated_script"] = script
            st.success("ìŠ¤í¬ë¦½íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")


# ============================================================
# ë¯¸ë¦¬ë“£ê¸° íƒ­
# ============================================================

def render_preview_tab():
    """ë¯¸ë¦¬ë“£ê¸° íƒ­"""
    st.markdown("### ğŸ§ ë¯¸ë¦¬ë“£ê¸°")

    # ë§ˆì§€ë§‰ ìƒì„± ì˜¤ë””ì˜¤
    if "last_tts_audio" in st.session_state and st.session_state["last_tts_audio"]:
        st.markdown("#### ğŸ”Š ë§ˆì§€ë§‰ ìƒì„± ì˜¤ë””ì˜¤")
        if os.path.exists(st.session_state["last_tts_audio"]):
            st.audio(st.session_state["last_tts_audio"])
        else:
            st.warning("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # ìƒì„± íˆìŠ¤í† ë¦¬
    st.markdown("#### ğŸ“œ ìƒì„± íˆìŠ¤í† ë¦¬")

    tts_dir = st.session_state.get("tts_output_dir", "data/tts")

    if os.path.exists(tts_dir):
        files = []
        for f in os.listdir(tts_dir):
            if f.endswith(('.wav', '.mp3')):
                files.append(f)

        files = sorted(
            files,
            key=lambda x: os.path.getmtime(os.path.join(tts_dir, x)),
            reverse=True
        )[:10]

        if files:
            for f in files:
                file_path = os.path.join(tts_dir, f)
                file_size = os.path.getsize(file_path) / 1024  # KB

                with st.expander(f"â–¶ï¸ {f} ({file_size:.1f} KB)"):
                    st.audio(file_path)

                    col1, col2 = st.columns(2)
                    with col1:
                        with open(file_path, "rb") as file:
                            st.download_button(
                                "ğŸ’¾ ë‹¤ìš´ë¡œë“œ",
                                data=file,
                                file_name=f,
                                use_container_width=True,
                                key=f"dl_{f}"
                            )
                    with col2:
                        if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{f}", use_container_width=True):
                            os.remove(file_path)
                            st.rerun()
        else:
            st.info("ìƒì„±ëœ ì˜¤ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ============================================================
# ë©”ì¸
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    st.title("ğŸ¤ TTS ìƒì„±")

    # í”„ë¡œì íŠ¸ ì„ íƒ
    col1, col2 = st.columns([3, 1])
    with col1:
        project = st.selectbox(
            "í”„ë¡œì íŠ¸ ì„ íƒ",
            options=["ì„¸ëª¨ì§€", "í”„ë¡œì íŠ¸ 2", "í”„ë¡œì íŠ¸ 3"],
            key="tts_project"
        )

    st.markdown("---")

    # === íƒ­ êµ¬ì„± ===
    tabs = st.tabs([
        "âš™ï¸ ì„¤ì •",
        "âœ¨ Edge TTS",
        "ğŸ¤ Chatterbox",
        "âœï¸ ìˆ˜ë™ ì…ë ¥",
        "ğŸ§ ë¯¸ë¦¬ë“£ê¸°"
    ])

    with tabs[0]:
        render_settings_tab()

    with tabs[1]:
        render_edge_tts_tab()

    with tabs[2]:
        render_chatterbox_tab()

    with tabs[3]:
        render_manual_input_tab()

    with tabs[4]:
        render_preview_tab()


if __name__ == "__main__":
    main()
